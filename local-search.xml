<?xml version="1.0" encoding="utf-8"?>
<search>
  
  
  
  <entry>
    <title>反向传播算法推导-全连接神经网络</title>
    <link href="/2024/09/05/%E5%8F%8D%E5%90%91%E4%BC%A0%E6%92%AD%E7%AE%97%E6%B3%95%E6%8E%A8%E5%AF%BC-%E5%85%A8%E8%BF%9E%E6%8E%A5%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/"/>
    <url>/2024/09/05/%E5%8F%8D%E5%90%91%E4%BC%A0%E6%92%AD%E7%AE%97%E6%B3%95%E6%8E%A8%E5%AF%BC-%E5%85%A8%E8%BF%9E%E6%8E%A5%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/</url>
    
    <content type="html"><![CDATA[<blockquote><p>https://zhuanlan.zhihu.com/p/39195266</p></blockquote><h1 id="算法的历史">算法的历史</h1><p>反向传播算法最早出现于1986年，用于解决多层神经网络的训练问题，由Rumelhart和Hinton等人提出，这篇论文当时发表在Nature上:</p><p>David E. Rumelhart, Geoffrey E. Hinton, and Ronald J. Williams.Learning internal representations by back-propagating errors. Nature,323(99): 533-536, 1986.</p><img src="/2024/09/05/%E5%8F%8D%E5%90%91%E4%BC%A0%E6%92%AD%E7%AE%97%E6%B3%95%E6%8E%A8%E5%AF%BC-%E5%85%A8%E8%BF%9E%E6%8E%A5%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/%E7%AE%97%E6%B3%95%E5%8E%86%E5%8F%B2.png" class="" title="算法历史"><h1 id="链式法则">链式法则</h1><p>在正式介绍神经网络的原理之前，先回顾一下多元函数求导的链式法则。对于如下的多元复合函数：</p><p><spanclass="math display">\[f\left(u\left(x,y\right),\nu\left(x,y\right),w\left(x\right)\right)\]</span></p><p>在这里，x和y是自变量。其中u，v，w是x的函数，u，v是y的函数，而f又是u，v，w的函数。根据链式法则，函数f对x和y的偏导数分别为：</p><p><span class="math display">\[\begin{aligned}&amp;\frac{\partialf}{\partial x}=\frac{\partial f}{\partial u}\frac{\partial u}{\partialx}+\frac{\partial f}{\partial\nu}\frac{\partial\nu}{\partialx}+\frac{\partial f}{\partial w}\frac{\partial w}{\partial x}\end{aligned}\]</span></p><p><span class="math display">\[\begin{aligned}&amp;\frac{\partialf}{\partial y}=\frac{\partial f}{\partial u}\frac{\partial u}{\partialy}+\frac{\partial f}{\partial\nu}\frac{\partial\nu}{\partialy}\end{aligned}\]</span></p><p>总结起来，函数自变量x的偏导数等于函数对它上一层的复合节点的偏导数（在这里是u，v，w）与这些节点对x的偏导数乘积之和。因此，我们要计算某一个自变量的偏导数，最直接的路径是找到它上一层的复合节点，根据这些节点的偏导数来计算。</p><h1 id="神经网络原理简介">神经网络原理简介</h1><p>在推导算法之前，我们首先简单介绍一下人工神经网络的原理。大脑的神经元通过突触与其他神经元相连接，接收其他神经元送来的信号，经过汇总处理之后产生输出。在人工神经网络中，神经元的作用和这类似。下图是一个神经元的示意图，左侧为输入数据，右侧为输出数据：</p><img src="/2024/09/05/%E5%8F%8D%E5%90%91%E4%BC%A0%E6%92%AD%E7%AE%97%E6%B3%95%E6%8E%A8%E5%AF%BC-%E5%85%A8%E8%BF%9E%E6%8E%A5%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E5%8E%9F%E7%90%861.png" class="" title="神经网络原理1"><p>这个神经元接受的输入信号为向量(x1,x2,x3,x4,x5)，向量(w1,w2,w3,w4,w5)为输入向量的组合权重，b为偏置项，是标量。神经元对输入向量进行加权求和，并加上偏置项，最后经过激活函数变换产生输出：</p><p><spanclass="math display">\[y=f\left(\sum_{i=1}^5w_ix_i+b\right)\]</span></p><p>为表述简洁，我们把公式写成向量和矩阵形式。对每个神经元，它接受的来自前一层神经元的输入为向量x，本节点的权重向量为w，偏置项为b，该神经元的输出值为：</p><p><spanclass="math display">\[f\left(\mathrm{w}^\mathrm{T}\mathrm{x}+b\right)\]</span></p><p>先计算输入向量与权重向量的内积，加上偏置项，再送入一个函数进行变换，得到输出。这个函数称为激活函数，典型的是sigmoid函数。</p><p>神经网络一般有多个层。第一层为输入层，对应输入向量，神经元的数量等于特征向量的维数，这个层不对数据进行处理，只是将输入向量送入下一层中进行计算。中间为隐含层，可能有多个。最后是输出层，神经元的数量等于要分类的类别数，输出层的输出值被用来做分类预测。</p><p>下面我们来看一个简单神经网络的例子，如下图所示：</p><img src="/2024/09/05/%E5%8F%8D%E5%90%91%E4%BC%A0%E6%92%AD%E7%AE%97%E6%B3%95%E6%8E%A8%E5%AF%BC-%E5%85%A8%E8%BF%9E%E6%8E%A5%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E5%8E%9F%E7%90%862.png" class="" title="神经网络原理2"><p>这个网络有3层。第一层是输入层，对应的输入向量为x，有3个神经元，写成分量形式为（x1,x2,x3）它不对数据做任何处理，直接原样送入下一层。中间层有4个神经元，接受的输入数据为向量x，输出向量为y，写成分量形式为(y1,y2,y3,y4,y5)。第三个层为输出层，接受的输入数据为向量y，输出向量为z，写成分量形式为(z1,z2)。第一层到第二层的权重矩阵为W(1)，第二层到第三层的权重矩阵为W(2)。权重矩阵的每一行为一个权重向量，是上一层所有神经元到本层某一个神经元的连接权重，这里的上标表示层数。</p><p>如果激活函数选用sigmoid函数，则第二层神经元的输出值为：</p><p><spanclass="math display">\[y_{1}=\frac{1}{1+\exp\left(-\left(w_{11}^{(1)}x_{1}+w_{12}^{(1)}x_{2}+w_{13}^{(1)}x_{3}+b_{1}^{(1)}\right)\right)}\]</span></p><p><spanclass="math display">\[y_{2}=\frac{1}{1+\exp\left(-\left(w_{21}^{(1)}x_{1}+w_{22}^{(1)}x_{2}+w_{23}^{(1)}x_{3}+b_{2}^{(1)}\right)\right)}\]</span></p><p><spanclass="math display">\[y_{3}=\frac{1}{1+\exp\left(-\left(w_{31}^{(1)}x_{1}+w_{32}^{(1)}x_{2}+w_{33}^{(1)}x_{3}+b_{3}^{(1)}\right)\right)}\]</span></p><p><spanclass="math display">\[y_{4}=\frac{1}{1+\exp\left(-\left(w_{41}^{(1)}x_{1}+w_{42}^{(1)}x_{2}+w_{43}^{(1)}x_{3}+b_{4}^{(1)}\right)\right)}\]</span></p><p>第三层神经元的输出值为：</p><p><spanclass="math display">\[z_1=\frac{1}{1+\exp\left(-\left(w_{11}^{(2)}y_1+w_{12}^{(2)}y_2+w_{13}^{(2)}y_3+w_{14}^{(2)}y_4+b_1^{(2)}\right)\right)}\]</span></p><p><spanclass="math display">\[z_2=\frac{1}{1+\exp\left(-\left(w_{21}^{(2)}y_1+w_{22}^{(2)}y_2+w_{23}^{(2)}y_3+w_{24}^{(2)}y_4+b_2^{(2)}\right)\right)}\]</span></p><p>如果把yi代入上面二式中，可以将输出向量z表示成输出向量x的函数。通过调整权重矩阵和偏置项可以实现不同的函数映射，因此神经网络就是一个复合函数。</p><p>需要解决的一个核心问题是一旦神经网络的结构（即神经元层数，每层神经元数量）确定之后，<strong>怎样得到权重矩阵和偏置项</strong>。这些参数是通过训练得到的，这是推导的核心任务。</p><h2 id="一个简单的例子">一个简单的例子</h2><p>首先以前面的3层神经网络为例，推导损失函数对神经网络所有参数梯度的计算方法。假设训练样本集中有m个样本(xi,zi)。其中x为输入向量，z为标签向量。现在要确定神经网络的映射函数：</p><p><spanclass="math display">\[\mathbf{z}=h\left(\mathbf{x}\right)\]</span></p><p>什么样的函数能很好的解释这批训练样本？答案是神经网络的预测输出要尽可能的接近样本的标签值，即在训练集上最小化预测误差。如果使用均方误差，则优化的目标为：</p><p><spanclass="math display">\[L=\frac1{2m}\sum_{i=1}^m\left\|h\left(x_i\right)-z_i\right\|^2\]</span></p><p>其中h(x)和<spanclass="math inline">\(z_i\)</span>都是向量，求和项内部是向量的2范数平方，即各个分量的平方和。上面的误差也称为欧氏距离损失函数，除此之外还可以使用其他损失函数，如交叉熵、对比损失等。</p><p>优化目标函数的自变量是各层的权重矩阵和梯度向量，一般情况下无法保证目标函数是凸函数，因此这不是一个凸优化问题，有陷入局部极小值和鞍点的风险，这是神经网络之前一直被诟病的一个问题。可以使用梯度下降法进行求解，使用梯度下降法需要计算出损失函数对所有权重矩阵、偏置向量的梯度值，接下来的关键是这些梯度值的计算。在这里我们先将问题简化，只考虑对单个样本的损失函数：</p><p><spanclass="math display">\[L=\frac12{\left\|h\left(x\right)-z\right\|}^2\]</span></p><p>后面如果不加说明，都使用这种单样本的损失函数。如果计算出了对单个样本损失函数的梯度值，对这些梯度值计算均值即可得到整个目标函数的梯度值。</p><p><span class="math inline">\(w^{(1)}\)</span>和<spanclass="math inline">\(w^{(2)}\)</span>要被代入到网络的后一层中，是复合函数的内层变量，我们先考虑外层的<spanclass="math inline">\(w^{(2)}\)</span>和<spanclass="math inline">\(b^{(2)}\)</span>。权重矩阵<spanclass="math inline">\(w^{(2)}\)</span>是一个2x4的矩阵，它的两个行分别为向量<spanclass="math inline">\(w^{(1)}\)</span>和<spanclass="math inline">\(w^{(2)}\)</span>，<spanclass="math inline">\(b^{(2)}\)</span>是一个2维的列向量，它的两个元素为<spanclass="math inline">\(b^{(1)}\)</span>和<spanclass="math inline">\(b^{(2)}\)</span>。网络的输入是向量x，第一层映射之后的输出是向量y。</p><p>首先计算损失函数对权重矩阵每个元素的偏导数，将欧氏距离损失函数展开，有：</p><p><span class="math display">\[\frac{\partial L}{\partialw_{ij}^{(2)}}=\frac{\partial\frac12\left(\left(f\left(\mathrm{w}_1^{(2)}\mathrm{y}+b_1^{(2)}\right)-z_1\right)^2+\left(f\left(\mathrm{w}_2^{(2)}\mathrm{y}+b_2^{(2)}\right)-z_2\right)^2\right)}{\partialw_{ij}^{(2)}}\]</span></p><p>如果i = 1，即对权重矩阵第一行的元素求导，上式分子中的后半部分对 <spanclass="math inline">\(w_{ij}\)</span> 来说是常数。根据链式法则有：</p><p><span class="math display">\[\begin{aligned}\frac{\partial L}{\partial w_{ij}^{(2)}}&amp;=\left(f\left(\mathrm{w}_1^{(2)}\mathrm{y}+b_1^{(2)}\right)-z_1\right)f^{&#39;}\left(\mathrm{w}_1^{(2)}\mathrm{y}+b_1^{(2)}\right)\frac{\partial\left(\mathrm{w}_1^{(2)}\mathrm{y}+b_1^{(2)}\right)}{\partialw_{ij}^{(2)}} \\&amp;=\left(f\left(\mathrm{w}_1^{(2)}\mathrm{y}+b_1^{(2)}\right)-z_1\right)f&#39;\left(\mathrm{w}_1^{(2)}\mathrm{y}+b_1^{(2)}\right)\frac{\partial\left(\sum_{k=1}^4w_{1k}^{(2)}y_k+b_1^{(2)}\right)}{\partialw_{ij}^{(2)}} \\&amp;=\left(f\left(\mathrm{w}_1^{(2)}\mathrm{y}+b_1^{(2)}\right)-z_1\right)f^{&#39;}\left(\mathrm{w}_1^{(2)}\mathrm{y}+b_1^{(2)}\right)y_j\end{aligned}\]</span></p><p>如果i = 2，即对矩阵第二行的元素求导，类似的有：</p><p><span class="math display">\[\frac{\partial L}{\partialw_{ij}^{(2)}}=\left(f\left(\mathrm{w}_2^{(2)}\mathrm{y}+b_2^{(2)}\right)-z_2\right)f^{\prime}\left(\mathrm{w}_2^{(2)}\mathrm{y}+b_2^{(2)}\right)y_j\]</span></p><p>可以统一写成：</p><p><span class="math display">\[\frac{\partial L}{\partialw_{ij}^{(2)}}=\left(f\left(\mathrm{w}_i^{(2)}\mathrm{y}+b_i^{(2)}\right)-z_i\right)f^{\prime}\left(\mathrm{w}_i^{(2)}\mathrm{y}+b_i^{(2)}\right)y_j\]</span></p><p>可以发现，第一个下标i决定了权重矩阵的第i行和偏置向量的第i个分量，第二个下标j决定了向量y的第j个分量。这可以看成是一个列向量与一个行向量相乘的结果，写成矩阵形式为：</p><p><spanclass="math display">\[\nabla_{\mathrm{W}^{(2)}}L=\left(f\left(\mathrm{W}^{(2)}\mathrm{y}+\mathrm{b}^{(2)}\right)-\mathrm{z}\right)\odotf^{\prime}\left(\mathrm{W}^{(2)}\mathrm{y}+\mathrm{b}^{(2)}\right)\mathrm{y}^{\mathrm{T}}\]</span></p><p>上式中乘法$<spanclass="math inline">\(为向量对应元素相乘，第二个乘法是矩阵乘法。\)</span>f(w<sup>{(2)}y+b</sup>{(2)})-z<spanclass="math inline">\(是一个二维向量，\)</span>f(w<sup>{(2)}y+b</sup>{(2)})<spanclass="math inline">\(也是一个二维向量，两个向量执行\)</span><spanclass="math inline">\(运算的结果还是一个 2 维列向量。y 是一个 4元素的列向量，其转置为 4维行向量，前面这个二维列向量与\)</span>y<sup>{T}<spanclass="math inline">\(的乘积为 2x4的矩阵，这正好与矩阵\)</span>w</sup>{(2)}<spanclass="math inline">\(的尺寸相等。在上面的公式中，权重的偏导数在求和项中由3部分组成，分别是网络输出值与真实标签值的误差\)</span>f(w<sup>{(2)}y+b</sup>{(2)})-z<spanclass="math inline">\(，激活函数的导数\)</span>f(w<sup>{(2)}y+b</sup>{(2)})$，本层的输入值y。神经网络的输出值、激活函数的导数值、本层的输入值都可以在正向传播时得到，因此可以高效的计算出来。对所有训练样本的偏导数计算均值，可以得到总的偏导数。</p><p>对偏置项的偏导数为：</p><p><spanclass="math display">\[\frac{\partial\left(\left(f\left(\mathbf{w}_1^{(2)}\mathbf{y}+b_1^{(2)}\right)-z_1\right)^2+\left(f\left(\mathbf{w}_2^{(2)}\mathbf{y}+b_2^{(2)}\right)-z_2\right)^2\right)}{\partialb_i^{(2)}}\]</span></p><p>如果i = 1，上式分子中的后半部分对<spanclass="math inline">\(b_1\)</span>来说是常数，有：</p><p><span class="math display">\[\begin{aligned}\frac{\partialL}{\partialb_{1}^{(2)}}&amp;=\left(f\left(\mathrm{w}_{1}^{(2)}\mathrm{y}+b_{1}^{(2)}\right)-z_{1}\right)f^{\prime}\left(\mathrm{w}_{1}^{(2)}\mathrm{y}+b_{1}^{(2)}\right)\frac{\partial\left(\mathrm{w}_{1}^{(2)}\mathrm{y}+b_{1}^{(2)}\right)}{\partialb_{1}^{(2)}}\\&amp;=\left(f\left(\mathrm{w}_{1}^{(2)}\mathrm{y}+b_{1}^{(2)}\right)-z_{1}\right)f^{\prime}\left(\mathrm{w}_{1}^{(2)}\mathrm{y}+b_{1}^{(2)}\right)\end{aligned}\]</span></p><p>如果i = 2，类似的有：</p><p><span class="math display">\[\frac{\partial L}{\partialb_2^{(2)}}=\left(f\left(\mathrm{w}_2^{(2)}\mathrm{y}+b_2^{(2)}\right)-z_2\right)f^{\prime}\left(\mathrm{w}_2^{(2)}\mathrm{y}+b_2^{(2)}\right)\]</span></p><p>这可以统一写成</p><p><span class="math display">\[\frac{\partial L}{\partialb_i^{(2)}}=\left(f\left(\mathrm{w}_i^{(2)}\mathrm{y}+b_i^{(2)}\right)-z_i\right)f^{\prime}\left(\mathrm{w}_i^{(2)}\mathrm{y}+b_i^{(2)}\right)\]</span></p><p>写成矩阵形式为：</p><p><spanclass="math display">\[\nabla_{\mathrm{b}^{(2)}}L=\left(f\left(\mathrm{W}^{(2)}\mathrm{y}+\mathrm{b}^{(2)}\right)-\mathrm{z}\right)\odotf^{\prime}\left(\mathrm{W}^{(2)}\mathrm{y}+\mathrm{b}^{(2)}\right)\]</span></p><p>偏置项的导数由两部分组成，分别是神经网络预测值与真实值之间的误差，激活函数的导数值，与权重矩阵的偏导数相比唯一的区别是少了<spanclass="math inline">\(y^{T}\)</span></p><p>接下来计算对<span class="math inline">\(w^{(1)}\)</span>和<spanclass="math inline">\(w^{(2)}\)</span>的偏导数，由于是复合函数的内层，情况更为复杂。<spanclass="math inline">\(w^{(1)}\)</span>是一个 4x3 的矩阵，它的 4个行向量为<spanclass="math inline">\(w_1^1,w_2^1,w_3^1,w_4^1\)</span>。偏置项<spanclass="math inline">\(b^{(1)}\)</span>是 4 维向量，4个分量分别是<spanclass="math inline">\(b_1^1,b_2^1,b_3^1,b_4^1\)</span>。首先计算损失函数对<spanclass="math inline">\(w^{(1)}\)</span>的元素的偏导数:</p><p><span class="math display">\[\frac{\partial L}{\partialw_{ij}^{(1)}}=\frac{\partial\frac12\left(\left(f\left(\mathrm{w}_1^{(2)}\mathrm{y}+b_1^{(2)}\right)-z_1\right)^2+\left(f\left(\mathrm{w}_2^{(2)}\mathrm{y}+b_2^{(2)}\right)-z_2\right)^2\right)}{\partialw_{ij}^{(1)}}\]</span></p><p>而</p><p><spanclass="math display">\[\mathrm{y}=f\left(\mathrm{W}^{(1)}\mathrm{x}+\mathrm{b}^{(1)}\right)\]</span></p><p>上式分子中的两部分都有y，因此都与<spanclass="math inline">\(w^{(1)}\)</span>有关。为了表述简洁，我们令：</p><p><spanclass="math display">\[\mathrm{u}^{(2)}=\mathrm{W}^{(2)}\mathrm{y}+\mathrm{b}^{(2)}\]</span></p><p>根据链式法则有</p><p><span class="math display">\[\frac{\partial L}{\partialw_{ij}^{(1)}}=\left(f\left(u_1^{(2)}\right)-z_1\right)f^{&#39;}\left(u_1^{(2)}\right)\frac{\partial\mathrm{w}_1^{(2)}\mathrm{y}}{\partialw_{ij}^{(1)}}+\left(f\left(u_2^{(2)}\right)-z_2\right)f^{&#39;}\left(u_2^{(2)}\right)\frac{\partial\mathrm{w}_2^{(2)}\mathrm{y}}{\partialw_{ij}^{(1)}}\]</span></p><p>其中<span class="math inline">\(f(u_1^2)-z_1\)</span>和<spanclass="math inline">\(f^{`}(u_{1}^{2})\)</span>，<spanclass="math inline">\(f(u_2^2)-z_2\)</span>和 <spanclass="math inline">\(f^{`}(u_{2}^{2})\)</span>都是标量，<spanclass="math inline">\(w_1^2y\)</span>和<spanclass="math inline">\(w_2^2y\)</span> 是两个向量的内积，y的每一个分量都是<spanclass="math inline">\(w_{ij}^{1}\)</span>的函数，接下来计算<spanclass="math inline">\(\frac{\alpha w_{1}^{(2)}y}{\alphaw_{ij}^{(1)}}\)</span>和<span class="math inline">\(\frac{\alphaw_{2}^{(2)}y}{\alpha w_{ij}^{(1)}}\)</span></p><p><spanclass="math display">\[\frac{\partial\mathbf{w}_1^{(2)}\mathbf{y}}{\partialw_{ij}^{(1)}}=\mathbf{w}_1^{(2)}\frac{\partial\mathbf{y}}{\partialw_{ij}^{(1)}}\]</span></p><p>这里的<span class="math inline">\(\frac{\alpha y}{\alphaw_{ij}^{(1)}}\)</span>是一个向量，表示y的每个分量分别对<spanclass="math inline">\(w_{ij}^{(1)}\)</span>求导。当i=1时有:</p><p><span class="math display">\[\frac{\partial\mathbf{y}}{\partialw_{ij}^{(1)}}=\begin{bmatrix}\frac{\partial y_1}{\partialw_{ij}^{(1)}}\\\frac{\partial y_2}{\partialw_{ij}^{(1)}}\\\frac{\partial y_3}{\partialw_{ij}^{(1)}}\\\frac{\partial y_4}{\partialw_{ij}^{(1)}}\end{bmatrix}=\begin{bmatrix}f&#39;\left(\mathrm{w}_1^{(1)}\mathrm{x}+b_1^{(1)}\right)x_j\\0\\0\\0\end{bmatrix}\]</span></p><p>后面3个分量相对于求导变量<spanclass="math inline">\(w_{ij}^{(1)}\)</span>都是常数。类似的当i =2时有：</p><p><span class="math display">\[\frac{\partial\mathbf{y}}{\partialw_{ij}^{(1)}}=\begin{bmatrix}\frac{\partial y_1}{\partialw_{ij}^{(1)}}\\\frac{\partial y_2}{\partialw_{ij}^{(1)}}\\\frac{\partial y_3}{\partialw_{ij}^{(1)}}\\\frac{\partial y_4}{\partialw_{ij}^{(1)}}\end{bmatrix}=\begin{bmatrix}0\\f&#39;&#39;\Big(\mathrm{w}_2^{(1)}\mathrm{x}+b_1^{(1)}\Big)x_j\\0\\0\end{bmatrix}\]</span></p><p>i = 3和i = 4时的结果以此类推。综合起来有：</p><p><spanclass="math display">\[\frac{\partial\mathrm{w}_1^{(2)}\mathrm{y}}{\partialw_{ij}^{(1)}}=w_{1i}^{(2)}f^{&#39;}\left(\mathrm{w}_i^{(1)}\mathrm{x}+b_i^{(1)}\right)x_j\]</span></p><p>同理有：</p><p><spanclass="math display">\[\frac{\partial\mathrm{w}_2^{(2)}\mathrm{y}}{\partialw_{ij}^{(1)}}=w_{2i}^{(2)}f&#39;\left(\mathrm{w}_i^{(1)}\mathrm{x}+b_i^{(1)}\right)x_j\]</span></p><p>如果令：</p><p><spanclass="math display">\[\mathbf{u}^{\begin{pmatrix}1\end{pmatrix}}=\mathbf{W}^{\begin{pmatrix}1\end{pmatrix}}\mathbf{x}+\mathbf{b}^{\begin{pmatrix}1\end{pmatrix}}\]</span></p><p>合并得到：</p><p><span class="math display">\[\begin{aligned}\frac{\partial L}{\partial w_{ij}^{(1)}}&amp;=\left(f\left(u_1^{(2)}\right)-z_1\right)f&#39;\left(u_1^{(2)}\right)\frac{\partial\mathrm{w}_1^{(2)}\mathrm{y}}{\partialw_{ij}^{(1)}}+\left(f\left(u_2^{(2)}\right)-z_2\right)f&#39;\left(u_2^{(2)}\right)\frac{\partial\mathrm{w}_2^{(2)}\mathrm{y}}{\partialw_{ij}^{(1)}} \\&amp;=\left(f\left(u_1^{(2)}\right)-z_1\right)f^{\prime}\left(u_1^{(2)}\right)w_{1i}^{(2)}f^{\prime}\left(u_1^{(1)}\right)x_j+\left(f\left(u_2^{(2)}\right)-z_2\right)f^{\prime}\left(u_2^{(2)}\right)w_{2i}^{(2)}f^{\prime}\left(u_2^{(1)}\right)x_j\\&amp;=\begin{bmatrix}w_{1i}^{(2)}&amp;w_{2i}^{(2)}\end{bmatrix}\left(\left(f\left(\mathbf{u}^{(2)}\right)-\mathbf{z}\right)\odotf&#39;\left(\mathbf{u}^{(2)}\right)\odotf&#39;\left(\mathbf{u}^{(1)}\right)\right)x_j\end{aligned}\]</span></p><p>写成矩阵形式为：</p><p><spanclass="math display">\[\nabla_{\mathrm{W}^{(1)}}L=\left(\mathrm{W}^{(2)}\right)^{\mathrm{T}}\left(\left(f\left(\mathrm{u}^{(2)}\right)-\mathrm{z}\right)\odotf^{\prime}\left(\mathrm{u}^{(2)}\right)\odotf^{\prime}\left(\mathrm{u}^{(1)}\right)\right)\mathrm{x}^{\mathrm{T}}\]</span></p><p>最后计算偏置项的偏导数：</p><p><span class="math display">\[\frac{\partial L}{\partialb_{i}^{(1)}}=\left(f\left(u_{1}^{(2)}\right)-z_{1}\right)f^{&#39;}\left(u_{1}^{(2)}\right)\frac{\partial\mathbf{w}_{1}^{(2)}\mathbf{y}}{\partialb_{i}^{(1)}}+\left(f\left(u_{2}^{(2)}\right)-z_{2}\right)f^{&#39;}\left(u_{2}^{(2)}\right)\frac{\partial\mathbf{w}_{2}^{(2)}\mathbf{y}}{\partialb_{i}^{(1)}}\]</span></p><p>类似的我们得到：</p><p><spanclass="math display">\[\frac{\partial\mathbf{w}_1^{(2)}\mathbf{y}}{\partialb_i^{(1)}}=w_{1i}^{(2)}f&#39;\left(\mathbf{w}_i^{(1)}\mathbf{x}+b_i^{(1)}\right)\]</span></p><p>合并后得到：</p><p><span class="math display">\[\begin{aligned}\frac{\partial L}{\partial b_i^{(1)}}&amp;=\left(f\left(u_{1}^{(2)}\right)-z_{1}\right)f^{&#39;}\left(u_{1}^{(2)}\right)\frac{\partial\mathbf{w}_{1}^{(2)}\mathbf{y}}{\partialb_{i}^{(1)}}+\left(f\left(u_{2}^{(2)}\right)-z_{2}\right)f^{&#39;}\left(u_{2}^{(2)}\right)\frac{\partial\mathbf{w}_{2}^{(2)}\mathbf{y}}{\partialb_{i}^{(1)}} \\&amp;=\left(f\left(u_{1}^{(2)}\right)-z_{1}\right)f^{&#39;}\left(u_{1}^{(2)}\right)w_{1i}^{(2)}f^{&#39;}\left(u_{1}^{(1)}\right)+\left(f\left(u_{2}^{(2)}\right)-z_{2}\right)f^{&#39;}\left(u_{2}^{(2)}\right)w_{2i}^{(2)}f^{&#39;}\left(u_{2}^{(1)}\right)\\&amp;=\left[\begin{array}{cc}w_{1i}^{(2)}&amp;w_{2i}^{(2)}\end{array}\right]\left(\left(f\left(\mathbf{u}^{(2)}\right)-\mathbf{z}\right)\odotf^{\prime}\left(\mathbf{u}^{(2)}\right)\odotf^{\prime}\left(\mathbf{u}^{(1)}\right)\right)\end{aligned}\]</span></p><p>写成矩阵形式为：</p><p><spanclass="math display">\[\nabla_{\mathrm{b}^{(1)}}L=\left(\mathrm{W}^{(2)}\right)^\mathrm{T}\left(\left(f\left(\mathrm{u}^{(2)}\right)-\mathrm{z}\right)\odotf^{\prime}\left(\mathrm{u}^{(2)}\right)\odotf^{\prime}\left(\mathrm{u}^{(1)}\right)\right)\]</span></p><p>至此，我得到了这个简单网络对所有参数的偏导数，接下来我们将这种做法推广到更一般的情况。从上面的结果可以看出一个规律，输出层的权重矩阵和偏置向量梯度计算公式中共用了<spanclass="math inline">\(f(u^{(2)}-z)\odotf^{&#39;}(u^{(2)})\)</span>。对于隐含层也有类似的结果。</p><h1 id="完整的算法">完整的算法</h1><p>根据上面的结论可以方便的推导出神经网络的求导公式。假设神经网络有<spanclass="math inline">\(n_l\)</span>层第l层神经元个数为<spanclass="math inline">\(s_l\)</span>。第 l 层从第 l-1层接收的输入向量为<spanclass="math inline">\(x^{(l-1)}\)</span>,本层的权重矩阵为<spanclass="math inline">\(w^{(l)}\)</span>，偏置向量为<spanclass="math inline">\(b^{(l)}\)</span>，输出向量为<spanclass="math inline">\(x^{(l)}\)</span>。该层的输出可以写成如下矩阵形式:</p><p><spanclass="math display">\[\begin{aligned}&amp;\mathbf{u}^{(l)}=\mathbf{W}^{(l)}\mathbf{X}^{(l-1)}+\mathbf{b}^{(l)}\\&amp;\mathbf{x}^{(l)}=f\left(\mathbf{u}^{(l)}\right)\end{aligned}\]</span></p><p>其中<span class="math inline">\(w^{(l)}\)</span>是<spanclass="math inline">\(s_l\times s_{l-1}\)</span>的矩阵，<spanclass="math inline">\(u^{(l)}\)</span>和<spanclass="math inline">\(b^{(l)}\)</span>是<spanclass="math inline">\(s_l\)</span>维的向量。神经网络一个层实现的变换如下图所示:</p><img src="/2024/09/05/%E5%8F%8D%E5%90%91%E4%BC%A0%E6%92%AD%E7%AE%97%E6%B3%95%E6%8E%A8%E5%AF%BC-%E5%85%A8%E8%BF%9E%E6%8E%A5%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/%E5%AE%8C%E6%95%B4%E7%AE%97%E6%B3%951.png" class="" title="完整算法1"><p>如果将神经网络按照各个层展开，最后得到一个深层的复合函数，将其代入欧氏距离损失函数，依然是一个关于各个层的权重矩阵和偏置向量的复合函数：</p><img src="/2024/09/05/%E5%8F%8D%E5%90%91%E4%BC%A0%E6%92%AD%E7%AE%97%E6%B3%95%E6%8E%A8%E5%AF%BC-%E5%85%A8%E8%BF%9E%E6%8E%A5%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/%E5%AE%8C%E6%95%B4%E7%AE%97%E6%B3%952.png" class="" title="完整算法2"><p>要计算某一层的权重矩阵和偏置向量的梯度，只能依赖于它紧贴着的外面那一层变量的梯度值，通过一次复合函数求导得到。</p><p>根据定义，<span class="math inline">\(w^{(l)}\)</span>和<spanclass="math inline">\(b^{(l)}\)</span>是目标函数的自变量，<spanclass="math inline">\(u^{(l)}\)</span>和<spanclass="math inline">\(x^{(l)}\)</span>可以看成是它们的函数。根据前面的结论，损失函数对权重矩阵的梯度为:</p><p><spanclass="math display">\[\nabla_{\mathrm{W}^{(l)}}L=\left(\nabla_{\mathrm{u}^{(l)}}L\right)\left(\mathrm{x}^{(l-1)}\right)^{\mathrm{T}}\]</span></p><p>对偏置向量的梯度为：</p><p><spanclass="math display">\[\nabla_{\mathfrak{b}^{(l)}}L=\nabla_{\mathfrak{u}^{(l)}}L\]</span></p><p>现在的问题是，梯度<spanclass="math inline">\(\bigtriangledown_{u^{(l)}}L\)</span>怎么计算?我们分两种情况讨论，如果第l层是输出层，在这里只考虑对单个样本的损失函数，根据上一节推导的结论，这个梯度为：</p><p><spanclass="math display">\[\nabla_{\mathrm{u}^{(l)}}L=\left(\nabla_{\mathrm{x}^{(l)}}L\right)\odotf^{&#39;}\left(\mathrm{u}^{(l)}\right)=\left(\mathrm{x}^{(l)}-\mathrm{y}\right)\odotf^{&#39;}\left(\mathrm{u}^{(l)}\right)\]</span></p><p>这就是输出层的神经元输出值与期望值之间的误差。这样我们得到输出层权重的梯度为：</p><p><spanclass="math display">\[\nabla_{\mathrm{W}^{(l)}}L=\left(\mathrm{x}^{(l)}-\mathrm{y}\right)\odotf^{\prime}\left(\mathrm{u}^{(l)}\right)\left(\mathrm{x}^{(l-1)}\right)^{\mathrm{T}}\]</span></p><p>等号右边第一个乘法是向量对应元素乘；第二个乘法是矩阵乘，在这里是列向量与行向量的乘积，结果是一个矩阵，尺寸刚好和权重矩阵相同。损失函数对偏置项的梯度为：</p><p><spanclass="math display">\[\nabla_{\mathfrak{b}^{(l)}}L=\left(\mathbf{x}^{(l)}-\mathbf{y}\right)\odotf^{\prime}\left(\mathbf{u}^{(l)}\right)\]</span></p><p>下面考虑第二种情况。如果第l+1层是隐含层，则有：</p><p><spanclass="math display">\[\mathbf{u}^{(l+1)}=\mathbf{W}^{(l+1)}\mathbf{x}^{(l)}+\mathbf{b}^{(l+1)}=\mathbf{W}^{(l+1)}f\left(\mathbf{u}^{(l)}\right)+\mathbf{b}^{(l+1)}\]</span></p><p>假设梯度<spanclass="math inline">\(\bigtriangledown_{u^{(l)}}L\)</span>已经求出，根据前面的结论，有:</p><p><spanclass="math display">\[\nabla_{\mathbf{u}^{(l)}}L=\left(\nabla_{\mathbf{x}^{(l)}}L\right)\odotf^{&#39;}\left(\mathbf{u}^{(l)}\right)=\left(\left(\mathbf{W}^{(l+1)}\right)^{\mathrm{T}}\nabla_{\mathbf{u}^{(l+1)}}L\right)\odotf^{&#39;}\left(\mathbf{u}^{(l)}\right)\]</span></p><p>这是一个递推的关系，通过<spanclass="math inline">\(\bigtriangledown_{u^{(l+1)}}L\)</span>可以计算出<spanclass="math inline">\(\bigtriangledown_{u^{(l)}}L\)</span>，递推的终点是输出层，而输出层的梯度值我们之前已经算出。由于根据<spanclass="math inline">\(\bigtriangledown_{u^{(l)}}L\)</span>可以计算出<spanclass="math inline">\(\bigtriangledown_{w^{(l)}}L\)</span>和<spanclass="math inline">\(\bigtriangledown_{b^{(l)}}L\)</span>，因此可以计算出任意层权重与偏置的梯度值。</p><p><spanclass="math display">\[\delta^{(l)}=\nabla_{\mathrm{u}^{(l)}}L=\begin{cases}\left(\mathrm{x}^{(l)}-\mathrm{y}\right)\odotf^{&#39;}\left(\mathrm{u}^{(l)}\right)&amp;l=n_l\\\\\left(\mathrm{W}^{(l+1)}\right)^\mathrm{T}\left(\delta^{(l+1)}\right)\odotf^{&#39;}\left(\mathrm{u}^{(l)}\right)&amp;l\neqn_l\end{cases}\]</span></p><p>向量<spanclass="math inline">\(\delta^{(l)}\)</span>的尺寸和本层神经元的个数相同。这是一个递推的定义，<spanclass="math inline">\(\delta^{(l)}\)</span>依赖于<spanclass="math inline">\(\delta^{(l+1)}\)</span>，递推的终点是输出层，它的误差项可以直接求出。</p><p>根据误差项可以方便的计算出对权重和偏置的偏导数。首先计算输出层的误差项，根据他得到权重和偏置项的梯度，这是起点；根据上面的递推公式，逐层向前，利用后一层的误差项计算出本层的误差项，从而得到本层权重和偏置项的梯度。</p><p>单个样本的反向传播算法在每次迭代时的流程为：</p><p>1.正向传播，利用当前权重和偏置值，计算每一层对输入样本的输出值</p><p>2.反向传播，对输出层的每一个节点计算其误差：</p><p><spanclass="math display">\[\delta^{(n_l)}=\left(\mathrm{x}^{(n_l)}-\mathrm{y}\right)\odotf^{\prime}\left(\mathrm{u}^{(n_l)}\right)\]</span></p><p>3.对于<spanclass="math inline">\(l=n_{l}-1,...,2\)</span>的各层，计算第l层每个节点的误差</p><p><spanclass="math display">\[\delta^{(l)}=\left(\mathrm{W}^{(l+1)}\right)^\mathrm{T}\delta^{(l+1)}\odotf^{\prime}\left(\mathrm{u}^{(l)}\right)\]</span></p><p>4.根据误差计算损失函数对权重的梯度值：</p><p><spanclass="math display">\[\nabla_{\mathrm{W}^{(l)}}L=\delta^{(l)}\left(\mathbf{X}^{(l-1)}\right)^{\mathrm{T}}\]</span></p><p>对偏置的梯度为：</p><p><spanclass="math display">\[\nabla_{\mathfrak{b}^{(l)}}L=\delta^{(l)}\]</span></p><p>5.用梯度下降法更新权重和偏置：</p><p><spanclass="math display">\[\begin{aligned}&amp;\mathbf{W}^{(l)}=\mathbf{W}^{(l)}-\eta\nabla_{\mathbf{W}^{(l)}}L\\&amp;\mathbf{b}^{(l)}=\mathbf{b}^{(l)}-\eta\nabla_{\mathbf{b}^{(l)}}L\end{aligned}\]</span></p><p>实现时需要在正向传播时记住每一层的输入向量<spanclass="math inline">\(x^{(l-1)}\)</span> ，本层的激活函数导数值<spanclass="math inline">\(f^{&#39;}\left(u^{(l)}\right)\)</span></p><p>神经网络的训练算法可以总结为:<br />复合函数求导 + 梯度下降法<br />训练算法有两个版本：批量模式和单样本模式。批量模式每次梯度下降法迭代时对所有样本计算损失函数值，计算出对这些样本的总误差，然后用梯度下降法更新参数；单样本模式是每次对一个样本进行前向传播，计算对该样本的误差，然后更新参数，它可以天然的支持增量学习，即动态的加入新的训练样本进行训练。</p><p>在数学中，向量一般是列向量，但在编程语言中，向量一般按行存储，即是行向量，因此实现时计算公式略有不同，需要进行转置。正向传播时的计算公式为：</p><p><spanclass="math display">\[\mathbf{u}^{(l)}=\mathbf{x}^{(l-1)}\mathbf{W}^{(l)}+\mathbf{b}^{(l)}\]</span></p><p>反向传播时的计算公式为：</p><p><spanclass="math display">\[\delta^{(l)}=\delta^{(l+1)}\left(\mathrm{W}^{(l+1)}\right)^\mathrm{T}\odotf^{\prime}\left(\mathrm{u}^{(l)}\right)\]</span></p><p>对权重矩阵的计算公式为：</p><p><span class="math display">\[\nabla_\text{W}L=\left(\mathrm{x}^{(l-1)}\right)^\mathrm{T}\delta^{(l)}\]</span></p><p>这些向量都是行向量。</p>]]></content>
    
    
    <categories>
      
      <category>强化学习</category>
      
    </categories>
    
    
    <tags>
      
      <tag>强化学习</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>强化学习PPO</title>
    <link href="/2024/09/04/%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0PPO/"/>
    <url>/2024/09/04/%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0PPO/</url>
    
    <content type="html"><![CDATA[<h1 id="强化学习介绍">强化学习介绍</h1><img src="/2024/09/04/%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0PPO/%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0%E5%8E%9F%E7%90%86%E5%9B%BE.png" class="" title="强化学习原理"><p><strong>Action Space：</strong> 可选择的动作，比如<spanclass="math inline">\(\{\mathrm{left,~up,~right}\}\)</span></p><p><strong>Policy：</strong>策略函数，输入State，输出Action的概率分布。一般用π表示</p><p><span class="math display">\[\pi(left|s_t)=0.1\]</span> <spanclass="math display">\[\pi(up|s_t)=0.2\]</span> <spanclass="math display">\[\pi(right|s_t)=0.7\]</span></p><p><strong>Trajectory :</strong> 轨迹，用<spanclass="math inline">\({\tau}\)</span>表示，一连串状态和动作的序列。Episode,Rollout。<spanclass="math inline">\(\{\mathrm{s_0,~a_0,~s_1,~a_1,...}\}\)</span> <spanclass="math display">\[s_{t+1}=f(s_t,a_t)\text{ 确定}\]</span></p><p><span class="math display">\[s_{t+1}=P(\cdot|s_t,a_t)\text{随机}\]</span></p><p><strong>Return：</strong>回报，从当前时间点到游戏结束的Reward的累积和。</p><p><strong>期望：</strong> 每个可能结果的概率与其结果值的乘积之和</p><p><span class="math display">\[\mathrm{E}(\mathrm{x})_{x\simp(x)}=\sum_{x}x*p(x)\approx\frac{1}{n}\sum_{i=1}^{n}x\quad x\simp(x)\]</span></p><p><strong>强化学习的目标：</strong>训练一个Policy神经网络π，在所有状态S下，给出相应的Action，得到Return的期望最大。</p><p>也可以说是：训练一个Policy神经网络π，在所有的Trajectory中，得到Return的期望最大。</p><p>下面用数学公式表达这个目标 <spanclass="math display">\[E(R(\tau))_{\tau\simP_\theta(\tau)}=\sum_{\tau}R(\tau)P_\theta(\tau)\]</span></p><p><span class="math inline">\({\tau}\)</span>服从分布<spanclass="math inline">\(P_\theta(\tau)\)</span>，<spanclass="math inline">\(P_\theta(\tau)\)</span>是我们要训练的策略网络的参数，我们期望在神经网络参数<spanclass="math inline">\(\theta\)</span>的作用下，<spanclass="math inline">\({\tau}\)</span>获得的Return的期望尽可能的大。</p><p>按照期望的定义，它等于所有的<spanclass="math inline">\({\tau}\)</span>获得的Return乘以<spanclass="math inline">\({\tau}\)</span>的概率的累加。可以用梯度上升的方法使得这个期望尽可能的大。</p><h1 id="策略梯度法-policy-gradient">策略梯度法 Policy gradient</h1><p>先计算梯度，我们只能改变神经网络的参数，而不能改变环境给的reward，所以我们对<spanclass="math inline">\(\theta\)</span>求梯度，并且根据log函数求导公式<spanclass="math inline">\(\nabla\log f(x)=\frac{\nablaf(x)}{f(x)}\)</span>可以化简</p><p><span class="math display">\[\begin{aligned}\nabla E(R(\tau))_{\tau\sim P_{\theta}(\tau)}&amp;=\nabla\sum_{\tau}R(\tau)P_{\theta}(\tau) \\&amp;=\sum_{\tau}R(\tau)\nabla P_{\theta}(\tau) \\&amp;=\sum_{\tau}R(\tau)\nablaP_{\theta}(\tau)\frac{P_{\theta}(\tau)}{P_{\theta}(\tau)} \\&amp;=\sum_{\tau}P_{\theta}(\tau)R(\tau)\frac{\nablaP_{\theta}(\tau)}{P_{\theta}(\tau)} \\&amp;=\sum_\tau P_\theta(\tau)R(\tau)\frac{\nablaP_\theta(\tau)}{P_\theta(\tau)} \\&amp;\approx\frac1N\sum_{n=1}^NR(\tau^n)\frac{\nablaP_\theta(\tau^n)}{P_\theta(\tau^n)} \\&amp;=\frac{1}{N}\sum_{n=1}^{N}R(\tau^{n})\nabla\logP_{\theta}(\tau^{n})\end{aligned}\]</span></p><p>这里我们认为，下一个状态是完全由当前状态和当前动作决定的，那么，一个<spanclass="math inline">\({\tau}\)</span>的概率<spanclass="math inline">\(P_{\theta}(\tau^{n})\)</span>就是在这个<spanclass="math inline">\({\tau}\)</span>里所有的state，和这些state下给出的action概率的连乘，log的连乘等于log的连加，整理后表达式如下：</p><p><span class="math display">\[\begin{aligned}&amp;=\frac{1}{N}\sum_{n=1}^{N}R(\tau^{n})\nabla\logP_{\theta}(\tau^{n}) \\&amp;=\frac{1}{N}\sum_{n=1}^{N}R(\tau^{n})\nabla\log\prod_{t=1}^{T_{n}}P_{\theta}(a_{n}^{t}|s_{n}^{t})\\&amp;=\frac1N\sum_{n=1}^NR(\tau^n)\sum_{t=1}^{T_n}\nabla\mathrm{log}P_\theta(a_n^t|s_n^t)\\&amp;=\frac1N\sum_{n=1}^N\sum_{t=1}^{T_n}R(\tau^n)\nabla\mathrm{log}P_\theta(a_n^t|s_n^t)\end{aligned}\]</span></p><p>这个表达式的含义是对所有可能的<spanclass="math inline">\({\tau}\)</span>期望最大的梯度，用这个梯度乘以学习率去更新神经网络的参数，这就是策略梯度Policygradient算法</p><p><spanclass="math display">\[\frac1N\sum_{n=1}^N\sum_{t=1}^{T_n}R(\tau^n)\mathrm{log}P_\theta(a_n^t|s_n^t)\]</span></p><p>我们去掉对梯度的求导，看整理后的表达式，也就是让这个表达式尽可能大的<spanclass="math inline">\(\theta\)</span>，也就是我们要求的神经网络的参数。</p><p>这个表达式的意义也很直观，由两部分构成，第一部分是一个<spanclass="math inline">\({\tau}\)</span>得到的return，第二部分是每一步根据当前的state做出action的概率，然后求对数。log底数为e，是单调递增的，也就是说，这个表达式的直观意义是，如果一个<spanclass="math inline">\({\tau}\)</span>得到的return是大于0的，那么就增大这个<spanclass="math inline">\({\tau}\)</span>下所有state下采取当前action的概率。</p><h3 id="实际训练策略网络的方法">实际训练策略网络的方法</h3><p>首先定义loss函数，在需要最大化的目标函数前面加上-，让优化器最小化它<spanclass="math display">\[\text{Loss=}-\frac1N\sum_{n=1}^N\sum_{t=1}^{T_n}R(\tau^n)\mathrm{log}P_\theta(a_n^t|s_n^t)\]</span></p><img src="/2024/09/04/%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0PPO/pg1.png" class="" title="pg1"><p>定义一个卷积神经网络，输入为state，经过卷积神经网络的处理，最后输出层有三个神经元，经过softmax层后代表三个动作的概率，这里的概率值就是<spanclass="math inline">\(P_\theta(a_n^t|s_n^t)\)</span></p><img src="/2024/09/04/%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0PPO/pg2.png" class="" title="pg2"><p>然后让这个神经网络连续玩n场游戏，得到n个<spanclass="math inline">\({\tau}\)</span>和n个最后的return值，这里return就是loss公式里的<spanclass="math inline">\(R(\tau^n)\)</span>。注意，我们对每一步的action都是按照概率进行采样的，不是选取最大值的，这样我们就得到loss公式中的所有值了。可以进行一个batch训练来更新Policy神经网络。<br /><img src="/2024/09/04/%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0PPO/pg3.png" class="" title="pg3"></p><p>更新网络-再玩n场游戏，采集数据-再训练一个batch，这种更新策略叫OnPolicy，也就是我们采集数据用的Policy和训练的Policy是同一个，这样的问题是我们大部分时间都在采集数据，训练非常慢，这也是PPO算法要解决的问题。</p><p><spanclass="math display">\[=\frac1N\sum_{n=1}^N\sum_{t=1}^{T_n}R(\tau^n)\nabla\mathrm{log}P_\theta(a_n^t|s_n^t)\]</span>这表达式的直观意义是，如果一个<spanclass="math inline">\({\tau}\)</span>得到的return是大于0的，那么就增大这个<spanclass="math inline">\({\tau}\)</span>里面所有状态下，采取当前动作的概率。</p><h3 id="pg算法的改进">pg算法的改进</h3><p>但这明显有改进的空间<br />- 1我们是否增大或减少在状态s下做动作a的概率，应该看做了这个动作之后到游戏结束累计的reward，而不是整个<spanclass="math inline">\({\tau}\)</span>累计的reward，因为一个动作只能影响它之后的reward，不能影响之前的。- 2一个动作是可以对接下来的reward产生影响，但它有可能只影响接下来的几步，而且影响会逐步衰减，后面的reward更多的是由它当时的动作影响。</p><p>针对这两点，我们修改公式。 <spanclass="math display">\[R(\tau^n)\to\sum_{t&#39;=t}^{T_n}\gamma^{t&#39;-t}r_{t&#39;}^n=R_t^n\]</span>首先，对reward求和，不是对整个<spanclass="math inline">\({\tau}\)</span>的reward进行求和，而是从当前步t到<spanclass="math inline">\({\tau}\)</span>结束的reward进行求和。第二点是引入衰减因子<spanclass="math inline">\(\gamma\)</span>，<spanclass="math inline">\(\gamma\)</span>小于1，距离当前步数t越远，当前动作对reward影响越小，呈指数衰减。下一步的reweard乘以<spanclass="math inline">\(\gamma\)</span>，下下步的reward就乘以<spanclass="math inline">\(\gamma^2\)</span>,依此类推，距离当前越远的reward受当前步动作影响越小。</p><p>修改后的return用<spanclass="math inline">\(R_t^n\)</span>表示，总的思想是让<spanclass="math inline">\(R_t^n\)</span>尽可能地表现当前动作对整个<spanclass="math inline">\({\tau}\)</span> return的影响，去掉其他因素。</p><img src="/2024/09/04/%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0PPO/pg4.png" class="" title="pg4"><p>还有一种情况会影响算法的稳定性，比如在好的局势下，无论做什么动作都能得到正的reward，那么算法就会增加所有动作的概率，但是这也会让训练很慢，最好是让相对好的动作概率增加，让相对差的动作概率减小，这样训练可以加快很多。解决办法是给所有动作的reward都减去一个baseline，这样就能反映这个动作相对其他动作的价值，对坏的局势也一样。</p><img src="/2024/09/04/%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0PPO/pg5.png" class="" title="pg5"><p>如果baseline选择合适，不管在好的局势还是坏的局势下，动作得到的reward有正有负。<span class="math display">\[\begin{aligned}&amp;=\frac1N\sum_{n=1}^N\sum_{t=1}^{T_n}R_t^n\nabla\mathrm{log}P_\theta(a_n^t|s_n^t)\\&amp;=\frac{1}{N}\sum_{n=1}^{N}\sum_{t=1}^{T_{n}}(R_{t}^{n}-B(s_{n}^{t}))\nabla\mathrm{log}P_{\theta}(a_{n}^{t}|s_{n}^{t})\end{aligned}\]</span></p><p>所以在公式里再减去一个baseline，这个baseline也需要用神经网络估算，这就是Actor-Critic算法，用来做动作的神经网络就是Actor，就像体操运动员，对体操运动员的动作进行打分的裁判，就是Critic网络，它用来评估Actor动作的好坏。</p><h2 id="r_tn-bs_nt与优势函数"><spanclass="math inline">\(R_{t}^{n}-B(s_{n}^{t})\)</span>与优势函数</h2><h3 id="action-value-function">Action-Value-Function</h3><p><spanclass="math inline">\(R_{t}^{n}\)</span>每次都是一次随机采样，方差很大，训练不稳定。<br /><span class="math inline">\(Q_\theta(s,a)\)</span>在states下，做出Action A，期望的回报。动作价值函数。</p><h3 id="state-value-function">State-Value-Function</h3><p><span class="math inline">\(V_\theta(s)\)</span>在states下，期望的回报。状态价值函数。</p><h3 id="advantage-function">Advantage Function</h3><p><spanclass="math inline">\(A_{\theta}(s,a)=Q_{\theta}(s,a)-V_{\theta}(s)\)</span>在state s下，最哦出Action a，比其他动作能带来多少优势。</p><p>原来公式中的<spanclass="math inline">\(R_{t}^{n}-B(s_{n}^{t})\)</span>实际上就是想表达优势函数的意思，有了优势函数之后对公式进行替换</p><p><spanclass="math display">\[\frac1N\sum_{n=1}^N\sum_{t=1}^{T_n}A_\theta(s_n^t,a_n^t)\nabla\mathrm{log}P_\theta(a_n^t|s_n^t)\]</span></p><h2 id="优势函数的计算">优势函数的计算</h2><p><spanclass="math display">\[A_{\theta}(s,a)=Q_{\theta}(s,a)-V_{\theta}(s)\]</span></p><p>动作价值函数和状态价值函数的关系是什么，可以看下面这个等式。</p><p><spanclass="math display">\[Q_{\theta}(s_{t},a)=r_{t}+\gamma*V_{\theta}(s_{t+1})\]</span></p><p>对于<spanclass="math inline">\(s^t\)</span>时刻做出动作a，期望得到的return值，就等于这一步得到的reward<span class="math inline">\(r_t\)</span>加上衰减系数<spanclass="math inline">\(\gamma\)</span>乘上下一个状态<spanclass="math inline">\(s_{t+1}\)</span>的状态价值函数。带入优势函数。<spanclass="math display">\[A_{\theta}(s_{t},a)=r_{t}+\gamma*V_{\theta}(s_{t+1})-V_{\theta}(s_{t})\]</span></p><p>优势函数只有状态价值函数了，这样由原来需要训练两个神经网络，拟合动作价值函数和状态价值函数，变成只需要训练一个，代表状态价值函数。</p><p>下面对状态价值函数也进行action和reward的采样</p><p><span class="math display">\[V_\theta(s_{t+1})\approxr_{t+1}+\gamma*V_\theta(s_{t+2})\]</span></p><p>对<spanclass="math inline">\({\tau}\)</span>下一步的action和reward进行采样，<spanclass="math inline">\(s_{t+1}\)</span>的状态价值函数就约等于<spanclass="math inline">\(r_{t+1}+\gamma*V_\theta(s_{t+2})\)</span><br />以此类推，我们可以对优势函数进行一步action和reward采样，也可以进行两步，三步，或者全部进行采样。<br /><img src="/2024/09/04/%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0PPO/pg6.png" class="" title="pg6"></p><p>采样的步数越多，方差越大，偏差越小；采样的步数越少，越多部分的return是由状态价值函数估算，越能反映return的期望，它的方差越小，但是系统偏差越大。</p><p>为了让式子表示简洁，定义一个中间变量<spanclass="math inline">\(\delta\)</span> <spanclass="math display">\[\delta_t^V=r_t+\gamma*V_\theta(s_{t+1})-V_\theta(s_t)\]</span></p><p><spanclass="math inline">\(\delta_t\)</span>表示在第t步执行特定动作带来的优势</p><p><spanclass="math display">\[\delta_{t+1}^{V}=r_{t+1}+\gamma*V_{\theta}(s_{t+2})-V_{\theta}(s_{t+1})\]</span>同理<spanclass="math inline">\(\delta_{t+1}\)</span>表示在第t+1步执行特定动作带来的优势</p><p>把优势函数的一步采样、两步采样等用<spanclass="math inline">\(\delta\)</span>表达式代入后可表示为</p><p><spanclass="math display">\[\begin{aligned}&amp;A_\theta^1(s_t,a)=\delta_t^V\\&amp;A_\theta^2(s_t,a)=\delta_t^V+\gamma\delta_{t+1}^V\\&amp;A_\theta^3(s_t,a)=\delta_t^V+\gamma\delta_{t+1}^V+\gamma^2\delta_{t+2}^V\\&amp;\text{:}\end{aligned}\]</span></p><h2 id="gae优势函数">GAE优势函数</h2><p>我们在优势函数采样的时候具体该采样几步，GAE的答案是全都要。</p><p><spanclass="math display">\[A_\theta^{GAE}(s_t,a)=(1-\lambda)(A_\theta^1+\lambda*A_\theta^2+\lambda^2A_\theta^3+\cdotp\cdotp\cdotp)\]</span></p><p>它通过优势函数的一步采样、两步采样等分配不同的权重，然后将他们的加和来表示最终的GAE优势函数。</p><p>例如<span class="math inline">\(\lambda=0.9\)</span>时<br /><spanclass="math display">\[A_\theta^{GAE}=0.1A_\theta^1+0.09A_\theta^2+0.081A_\theta^3+\cdotp\cdotp\cdotp\]</span> 把<spanclass="math inline">\(\delta\)</span>代入GAE优势的表达式并整理，用等比数列求和公式化简得。</p><p><span class="math display">\[\begin{aligned}&amp;=(1-\lambda)(\delta_t^V+\lambda*(\delta_t^V+\gamma\delta_{t+1}^V)+\lambda^2(\delta_t^V+\gamma\delta_{t+1}^V+\gamma^2\delta_{t+2}^V)+\cdotp\cdotp\cdotp)\\&amp;=(1-\lambda)(\delta_t^V(1+\lambda+\lambda^2+\cdotp\cdotp\cdotp)+\gamma\delta_{t+1}^V*(\lambda+\lambda^2+\cdotp\cdotp\cdotp)+\cdotp\cdotp\cdotp)\\&amp;=(1-\lambda)(\delta_t^V\frac{1}{1-\lambda}+\gamma\delta_{t+1}^V\frac{\lambda}{1-\lambda}+\cdots)\\&amp;=\sum_{b=0}^\infty\left(\gamma\lambda\right)^b\delta_{t+b}^V\end{aligned}\]</span></p><p>它表示在状态<spanclass="math inline">\(s_t\)</span>时，做动作a的优势，并且平衡了采样不同步带来的方差和偏差的问题。</p><p>最终得到下面几个公式： <spanclass="math display">\[\delta_t^V=r_t+\gamma*V_\theta(s_{t+1})-V_\theta(s_t)\]</span></p><p><spanclass="math display">\[A_\theta^{GAE}(s_t,a)=\sum_{b=0}^\infty\left(\gamma\lambda\right)^b\delta_{t+b}^V\]</span></p><p><spanclass="math display">\[\frac1N\sum_{n=1}^N\sum_{t=1}^{T_n}A_\theta^{GAE}(s_n^t,a_n^t)\nabla\mathrm{log}P_\theta(a_n^t|s_n^t)\]</span>目的是让优化目标函数的值越大越好</p><p>这里的状态价值函数<spanclass="math inline">\(V_\theta(s_t)\)</span>用一个神经网络来拟合，一般可以和策略网络共用网络参数，只是最后一层不同，状态价值函数只需要输出一个单一的值来代表当前状态的价值即可。</p><img src="/2024/09/04/%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0PPO/pg7.png" class="" title="pg7"><p>价值网络的训练，就是统计当前步到这个<spanclass="math inline">\({\tau}\)</span>结束，所有的reward的衰减加和作为label，衰减系数通过<spanclass="math inline">\(\gamma\)</span>来控制，让价值网络拟合这个return值即可。</p><h1 id="ppo">PPO</h1><p>off policy<br /><img src="/2024/09/04/%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0PPO/ppo1.png" class="" title="ppo1"></p><img src="/2024/09/04/%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0PPO/ppo2.png" class="" title="ppo2"><h2 id="重要性采样">重要性采样</h2><p><span class="math display">\[E(f(x))_{x\simp(x)}=\sum_{x}f(x)*p(x)\]</span></p><p>想要求<span class="math inline">\(f(x)\)</span>的期望，x服从分布<spanclass="math inline">\(p(x)\)</span>，根据期望公式，等于对每一个可能的x，求<spanclass="math inline">\(f(x)*p(x)\)</span>再累加。</p><p><span class="math display">\[\begin{gathered}=\sum_xf(x)*p(x)\frac{q(x)}{q(x)} \\=\sum_xf(x)\frac{p(x)}{q(x)}*q(x) \\=\operatorname{E}(f(x)\frac{p(x)}{q(x)})_{x\sim q(x)} \\\approx\frac1N\sum_{n=1}^Nf(x)\frac{p(x)}{q(x)}_{x\sim q(x)}\end{gathered}\]</span> 可以转换为x在q分布下采样的公式。</p><p>用重要性采样更新目标函数的梯度公式。</p><p><span class="math display">\[\begin{aligned}&amp;\frac1N\sum_{n=1}^N\sum_{t=1}^{T_n}A_\theta^{GAE}(s_n^t,a_n^t)\nabla\mathrm{log}P_\theta(a_n^t|s_n^t)\\&amp;=\frac{1}{N}\sum_{n=1}^{N}\sum_{t=1}^{T_{n}}A_{\theta&#39;}^{GAE}(s_{n}^{t},a_{n}^{t})\frac{P_{\theta}(a_{n}^{t}|s_{n}^{t})}{P_{\theta&#39;}(a_{n}^{t}|s_{n}^{t})}\nabla\mathrm{log}P_{\theta}(a_{n}^{t}|s_{n}^{t})\end{aligned}\]</span></p><p>这样就可以用off policy的训练来代替on policy的训练。</p><p>用一个参考的策略<spanclass="math inline">\(\theta&#39;\)</span>来采样，计算参考策略<spanclass="math inline">\(\theta&#39;\)</span>的优势函数，然后用它来更新训练策略<spanclass="math inline">\(\theta\)</span>。只需要在公式里加上重要性采样的系数<spanclass="math inline">\(\frac{P_{\theta}(a_{n}^{t}|s_{n}^{t})}{P_{\theta&#39;}(a_{n}^{t}|s_{n}^{t})}\)</span>。</p><p>这个公式也很符合直觉，用老师教育学生的例子来理解<spanclass="math inline">\(\theta&#39;\)</span>就是小明的策略，<spanclass="math inline">\(\theta\)</span>是你的策略，<spanclass="math inline">\(\theta&#39;\)</span>的优势函数就是老师对小明的批评或表扬，你不能直接用老师对小明言行评价来更新你自己的言行准则。</p><p>根据log函数的求导公式可化简为 <spanclass="math display">\[\begin{aligned}&amp;=\frac{1}{N}\sum_{n=1}^{N}\sum_{t=1}^{T_{n}}A_{\theta&#39;}^{GAE}(s_{n}^{t},a_{n}^{t})\frac{P_{\theta}(a_{n}^{t}|s_{n}^{t})}{P_{\theta&#39;}(a_{n}^{t}|s_{n}^{t})}\frac{\nablaP_{\theta}(a_{n}^{t}|s_{n}^{t})}{P_{\theta}(a_{n}^{t}|s_{n}^{t})} \\&amp;=\frac1N\sum_{n=1}^N\sum_{t=1}^{T_n}A_{\theta^\prime}^{GAE}(s_n^t,a_n^t)\frac{\nablaP_\theta(a_n^t|s_n^t)}{P_{\theta^\prime}(a_n^t|s_n^t)}\end{aligned}\]</span></p><p>这个公式是梯度公式，我们去掉求梯度，在前面加上负号，就是PPO算法的loss函数。<br /><spanclass="math display">\[\mathrm{Loss}=-\frac1N\sum_{n=1}^N\sum_{t=1}^{T_n}A_{\theta^{\prime}}^{GAE}(s_n^t,a_n^t)\frac{P_\theta(a_n^t|s_n^t)}{P_{\theta^{\prime}}(a_n^t|s_n^t)}\]</span></p><p>可以看到它是用参考的策略<spanclass="math inline">\(\theta&#39;\)</span>来进行数据采样，计算优势函数，用训练的策略<spanclass="math inline">\(\theta\)</span>做某个动作的概率除以参考策略<spanclass="math inline">\(\theta&#39;\)</span>做某个动作的概率来调整优势函数。这样就可以用参考策略来进行数据采样，并且采样的数据可以多次用来训练policy网络，这样就解决了onpolicy训练效率太低的问题。</p><p>但是参考的策略不能和训练的策略在同一情况下给出各种动作概率分布的差别太大。例如老师教学生，这个学生不能和你差别太大，不然你很难学到对你有用的经验和教训。</p><p>为了解决这个问题，加上KL散度的约束。</p><p><spanclass="math display">\[Loss_{ppo}=-\frac{1}{N}\sum_{n=1}^{N}\sum_{t=1}^{T_{n}}A_{\theta&#39;}^{GAE}(s_{n}^{t},a_{n}^{t})\frac{P_{\theta}(a_{n}^{t}|s_{n}^{t})}{P_{\theta&#39;}(a_{n}^{t}|s_{n}^{t})}+\betaKL(P_{\theta},P_{\theta&#39;})\]</span></p><p>还有一种方法是通过戒断函数clip来替代KL散度，防止参考的策略和训练的策略偏差过大。</p><p><spanclass="math display">\[Loss_{ppo2}=-\frac1N\sum_{n=1}^N\sum_{t=1}^{T_n}min(A_{\theta^{\prime}}^{GAE}(s_n^t,a_n^t)\frac{P_\theta(a_n^t|s_n^t)}{P_{\theta^{\prime}}(a_n^t|s_n^t)},clip(\frac{P_\theta(a_n^t|s_n^t)}{P_{\theta^{\prime}}(a_n^t|s_n^t)},1-\varepsilon,1+\varepsilon)A_{\theta^{\prime}}^{GAE}(s_n^t,a_n^t))\]</span></p>]]></content>
    
    
    <categories>
      
      <category>强化学习</category>
      
    </categories>
    
    
    <tags>
      
      <tag>强化学习</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Actor-Critic</title>
    <link href="/2024/06/25/Actor-Critic/"/>
    <url>/2024/06/25/Actor-Critic/</url>
    
    <content type="html"><![CDATA[<h2 id="回顾Q-learning"><a href="#回顾Q-learning" class="headerlink" title="回顾Q_learning"></a>回顾Q_learning</h2><p><strong>本质:</strong> 估计当前状态s采取某个动作a后会获得的未来奖励的期望，即 Q(s,a)。  </p><p>将估计 Q(s,a)的过程看成一个<strong>评论家(Critic)</strong> ，它会对我们在当前状态s下，采取动作a的决策作出评价评价的结果就是 Q(s,a)的值。</p><img src="/2024/06/25/Actor-Critic/q_learning1.png" class="" title="q_learning1">  <h2 id="回顾Policy-Gradient"><a href="#回顾Policy-Gradient" class="headerlink" title="回顾Policy Gradient"></a>回顾Policy Gradient</h2><p><strong>核心思想:</strong> 根据当前状态，直接算出:  </p><ul><li>下一个动作是什么(确定性策略)</li><li>下一个动作的概率分布是什么(非确定性策略)</li></ul><img src="/2024/06/25/Actor-Critic/policy_gradient.png" class="" title="policy_gradient">  <h1 id="演员-评论家-Actor-Critic-算法"><a href="#演员-评论家-Actor-Critic-算法" class="headerlink" title="演员-评论家(Actor-Critic)算法"></a>演员-评论家(Actor-Critic)算法</h1><p><strong>Actor</strong> 就是 Policy Gradient，能够在<strong>连续动作空间</strong>中选择合适的动作  </p><ul><li>Policy Gradient 具有<strong>较高的采样方差</strong></li><li>Policy Gradient 是回合更新，学习效率较低</li></ul><p><strong>Critic</strong> 就是 Q-Learning(如深度 Q 网络)，能<strong>估计期望奖励</strong>，能进行单步更新  </p><ul><li>Q-Learning 等基于价值的算法只能用于离散动作空间</li></ul><p><strong>优势互补:</strong> Actor-Critic 结合了基于价值(Q-Learning)和基于策略(Policy Gradient)的两类强化学习算法</p><h3 id="策略梯度"><a href="#策略梯度" class="headerlink" title="策略梯度"></a>策略梯度</h3><img src="/2024/06/25/Actor-Critic/AC1.png" class="" title="AC1"> <p>用价值函数表示基线b，于是Actor Critic的策略梯度变为  </p><img src="/2024/06/25/Actor-Critic/AC2.png" class="" title="AC2"> <p>有三个网络需要学习，增大了估计不准的风险</p><p>利用<strong>贝尔曼方程</strong>建立 Q 和 V 的联系，省去一个网络。</p><p>贝尔曼方程是强化学习的基础和核心，用于求解马尔可夫决策过程（MDP）问题，也就是找到最优策略及其对应的价值函数，并建立起<strong>价值函数之间的关联</strong>。</p><h3 id="贝尔曼方程（Bellman-Equation）"><a href="#贝尔曼方程（Bellman-Equation）" class="headerlink" title="贝尔曼方程（Bellman Equation）"></a>贝尔曼方程（Bellman Equation）</h3><p>动作和状态价值函数的关系  </p><img src="/2024/06/25/Actor-Critic/AC3.png" class="" title="AC3"> <p>r”是状态s采取动作 a转移到新状态 s’的立即回报;<br>P”是状态s转移到新状态 s’的转移概率.</p><p>利用贝尔曼方程建立 Q 和 V 的联系，省去一个网络  </p><p>实践发现，忽略期望E也能取得很好的效果，且能建立 Q 函数和价值函数 V 的直接联系</p><p>原策略梯度：  </p><img src="/2024/06/25/Actor-Critic/AC4.png" class="" title="AC4">  <p>新策略梯度：  </p><img src="/2024/06/25/Actor-Critic/AC5.png" class="" title="AC5">  <h1 id="优势演员-评论家（Advantage-Actor-Critic，A2C）算法"><a href="#优势演员-评论家（Advantage-Actor-Critic，A2C）算法" class="headerlink" title="优势演员-评论家（Advantage Actor-Critic，A2C）算法"></a>优势演员-评论家（Advantage Actor-Critic，A2C）算法</h1><img src="/2024/06/25/Actor-Critic/A2C1.png" class="" title="A2C1">  <img src="/2024/06/25/Actor-Critic/A2C2.png" class="" title="A2C2">  <p>技巧 1-共享权重：Actor 和 Critic 网络有相同的输入，可以共享同一个特征提取网络。  </p><ul><li>降低计算开销、提高算法收敛性</li></ul><img src="/2024/06/25/Actor-Critic/A2C3.png" class="" title="A2C3">  <p>技巧 2-探索与利用：约束动作的概率分布，使得分布的熵不要太小，以尝试不同的动作，避免陷入局部极优。</p><img src="/2024/06/25/Actor-Critic/A2C4.png" class="" title="A2C4">  ]]></content>
    
    
    <categories>
      
      <category>强化学习</category>
      
    </categories>
    
    
    <tags>
      
      <tag>强化学习</tag>
      
      <tag>A2C</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>DQN</title>
    <link href="/2024/05/17/DQN/"/>
    <url>/2024/05/17/DQN/</url>
    
    <content type="html"><![CDATA[<h1 id="DQN"><a href="#DQN" class="headerlink" title="DQN"></a>DQN</h1><p>在 Q learning 中，我们有一个 Q table，记录着在每一个状态下，各个动作的 Q值。  </p><p>Q table 的作用是当我们输入状态 S，我们通过査表返回能够获得最大 Q 值的动作 A。也就是我们需要找一个 S-A 的对应关系。  </p><p>这种方式很适合格子游戏。因为格子游戏中的每一个格子就是一个状态，但在现实生活中，很多状态并不是离散而是连续的。  </p><p>例如在 GYM 中经典的 Cart Pole 游戏，杆子的角度是连续而不是离散的。在 Atari 游戏中，状态也是连续的。遇到这些情况，Q table 就没有办法解决。  </p><p>我们刚才说了 Q table 的作用就是找一个 S-A的对应关系。所以我们就可以用一个函数F表示，我们有 F(S)&#x3D;A。这样我们就可以不用查表了，而且还有个好处，函数允许连续状态的表示。这时候，我们深度神经网络就可以派上用场了。  </p><p>Deep network + Q learning &#x3D; DQN  </p><p>其实 Q learning 和 DQN 并没有根本的区别。只是 DQN 用神经网络，也就是一个函数替代了原来 Q table 而已。  </p><p>提到神经网络同学们并不陌生，NN 它是需要有 target 目标的。其实就和 Qlearning一样。还记得吗?在 Q learning，我们用下一状态 St+1 的最大 Q值替代 St+1的V值。V(St+1)加上状态转移产生的奖励 R。就是 Q(S,a)的更新目标。  </p><img src="/2024/05/17/DQN/DQN1.png" class="" title="DQN1">   <p>区别仅仅是这里不是更新Q table，而是更新Q网络。</p><img src="/2024/05/17/DQN/DQN2.png" class="" title="DQN2">   <ol><li>执行 A，往前一步，到达 St+1;</li><li>把 St+1 输入 Q 网络，计算 St+1 下所有动作的 Q值;</li><li>获得最大的 Q 值加上奖励 R作为更新目标;</li><li>计算损失- Q(S,A)相当于有监督学习中的需要拟合的几率 logits，就是z</li><li>maxQ(St+1)+ R 相当于有监督学习中的labels，y true</li><li>用 mse 函数，得出两者的 loss</li><li>用 loss 更新 Q 网络。</li></ol>]]></content>
    
    
    <categories>
      
      <category>强化学习</category>
      
    </categories>
    
    
    <tags>
      
      <tag>强化学习</tag>
      
      <tag>DQN</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Q-learning算法实例1</title>
    <link href="/2024/05/13/Q-learning%E7%AE%97%E6%B3%95%E5%AE%9E%E4%BE%8B1/"/>
    <url>/2024/05/13/Q-learning%E7%AE%97%E6%B3%95%E5%AE%9E%E4%BE%8B1/</url>
    
    <content type="html"><![CDATA[<h1 id="Q-learning实例1："><a href="#Q-learning实例1：" class="headerlink" title="Q-learning实例1："></a>Q-learning实例1：</h1><p>环境为一个直长廊，直长廊正前方，有一个钻石，直长廊后面，是一个深渊。此处一个Agent，可选择的action有两个，分别是left和right。设定：掉进深渊，score为-1，得到钻石，score为+1 ，其余score均为0 。  </p><p>可以根据下方图画理解：</p><img src="/2024/05/13/Q-learning%E7%AE%97%E6%B3%95%E5%AE%9E%E4%BE%8B1/q%E5%AD%A6%E4%B9%A0%E7%AE%97%E6%B3%95%E5%AE%9E%E4%BE%8B1.png" class="" title="q学习算法实例1">   <p>如图，这个长得像小人的，就是Agent，它左侧阴影部分，就是深渊，掉进去就-1；离它较远的右侧，有一个粉红的球，就是钻石，得到就是+1。  </p><p>我们可以画一个表格，来抽象化一下环境和奖励：</p><table><thead><tr><th>深渊-1</th><th>Agent起始位置</th><th>0</th><th>0</th><th>0</th><th>0</th><th>钻石+1</th></tr></thead><tbody><tr><td>0</td><td>1</td><td>2</td><td>3</td><td>4</td><td>5</td><td>6</td></tr></tbody></table><p>这个表格第一行就是地图，第二行是地图下标。</p><p>我们规定，Agent有只有两种动作，Left和Right，也就是左和右。</p><p>代码大致流程可以写出：</p><p>1、初始化，初始化环境参数、Agent参数；</p><p>2、图画更新，为便于用户观察，动态展示Agent位置；</p><p>3、Agent观察环境，看是否经历过这个state，如果经历过就选最优action，如果没有就随机action；</p><p>4、执行3所选的action；</p><p>5、观察终点，看是否到终点或是否掉入深渊；</p><p>6、更新坐标；</p><p>7、获取下一环境；</p><p>8、学习；</p><p>9、参数归零；</p><p>10、2-9迭代，直到5成立。</p><p>首先是环境代码：</p><p>Env.py</p><p>这是环境代码，主要功能包括：生成图像、获取所在位置、检测是否到终点、更新当前位置、获取下一步的实际情况和初始化地图</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">import</span> numpy <span class="hljs-keyword">as</span> np<br> <br> <br><span class="hljs-keyword">class</span> <span class="hljs-title class_">Env</span>:<span class="hljs-comment"># 定义环境</span><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">__init__</span>(<span class="hljs-params">self, column, start_colum, maze_column</span>):<br>        self.column = column  <span class="hljs-comment"># 表示地图的长度</span><br>        self.maze_column = maze_column - <span class="hljs-number">1</span>  <span class="hljs-comment"># 宝藏所在的位置</span><br>        self.x = start_colum  <span class="hljs-comment"># 初始化x</span><br>        self.<span class="hljs-built_in">map</span> = np.arange(column)  <span class="hljs-comment"># 给予每个地点一个标号</span><br>        self.count = <span class="hljs-number">0</span>  <span class="hljs-comment"># 用于记录一共走了多少步</span><br> <br>    <span class="hljs-comment"># 生成图像</span><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">draw</span>(<span class="hljs-params">self</span>):<br>        a = []<br>        <span class="hljs-keyword">for</span> j <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(self.column):  <span class="hljs-comment"># 更新图画</span><br>            <span class="hljs-keyword">if</span> j == <span class="hljs-number">0</span>:<br>                a.append(<span class="hljs-string">&#x27;x&#x27;</span>)<br>            <span class="hljs-keyword">elif</span> j == self.x:<br>                a.append(<span class="hljs-string">&#x27;o&#x27;</span>)<br>            <span class="hljs-keyword">elif</span> j == self.maze_column:<br>                a.append(<span class="hljs-string">&#x27;m&#x27;</span>)<br>            <span class="hljs-keyword">else</span>:<br>                a.append(<span class="hljs-string">&#x27;_&#x27;</span>)<br>        interaction = <span class="hljs-string">&#x27;&#x27;</span>.join(a)<br>        <span class="hljs-built_in">print</span>(<span class="hljs-string">&#x27;\r&#123;&#125;&#x27;</span>.<span class="hljs-built_in">format</span>(interaction), end=<span class="hljs-string">&#x27;&#x27;</span>)<br> <br>    <span class="hljs-comment"># 获取所在位置</span><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">get_observation</span>(<span class="hljs-params">self</span>):<br>        <span class="hljs-keyword">return</span> self.<span class="hljs-built_in">map</span>[self.x]  <span class="hljs-comment"># 返回现在所在位置</span><br> <br>    <span class="hljs-comment"># 是否已到达终点</span><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">get_terminal</span>(<span class="hljs-params">self</span>):<br>        <span class="hljs-keyword">if</span> self.x == self.maze_column:  <span class="hljs-comment"># 如果得到了宝藏，则返回已经完成</span><br>            done = <span class="hljs-literal">True</span><br>        <span class="hljs-keyword">elif</span> self.x == <span class="hljs-number">0</span>:  <span class="hljs-comment"># 如果掉入左边边缘，失败，-1</span><br>            done = <span class="hljs-literal">True</span><br>        <span class="hljs-keyword">else</span>:<br>            done = <span class="hljs-literal">False</span><br>        <span class="hljs-keyword">return</span> done<br> <br>    <span class="hljs-comment"># 更新当前位置</span><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">update_place</span>(<span class="hljs-params">self, action</span>):<br>        self.count += <span class="hljs-number">1</span>  <span class="hljs-comment"># 更新的时候表示已经走了一步</span><br>        <span class="hljs-keyword">if</span> action == <span class="hljs-string">&#x27;right&#x27;</span>:<br>            <span class="hljs-keyword">if</span> self.x &lt; self.column - <span class="hljs-number">1</span>:<br>                self.x += <span class="hljs-number">1</span><br>        <span class="hljs-keyword">elif</span> action == <span class="hljs-string">&#x27;left&#x27;</span>:  <span class="hljs-comment"># left</span><br>            <span class="hljs-keyword">if</span> self.x &gt; <span class="hljs-number">0</span>:<br>                self.x -= <span class="hljs-number">1</span><br> <br>    <span class="hljs-comment"># 获得下一步的环境的实际情况</span><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">get_target</span>(<span class="hljs-params">self, action</span>):<br>        <span class="hljs-keyword">if</span> action == <span class="hljs-string">&#x27;right&#x27;</span>:  <span class="hljs-comment"># 获得下一步的环境的实际情况</span><br>            <span class="hljs-keyword">if</span> self.x + <span class="hljs-number">1</span> == self.maze_column:<br>                score = <span class="hljs-number">1</span><br>                pre_done = <span class="hljs-literal">True</span><br>            <span class="hljs-keyword">else</span>:<br>                score = <span class="hljs-number">0</span><br>                pre_done = <span class="hljs-literal">False</span><br>            <span class="hljs-keyword">return</span> self.<span class="hljs-built_in">map</span>[self.x + <span class="hljs-number">1</span>], score, pre_done<br>        <span class="hljs-keyword">elif</span> action == <span class="hljs-string">&#x27;left&#x27;</span>:  <span class="hljs-comment"># left</span><br>            <span class="hljs-keyword">if</span> self.x - <span class="hljs-number">1</span> == self.maze_column:<br>                score = <span class="hljs-number">1</span><br>                pre_done = <span class="hljs-literal">True</span><br>            <span class="hljs-keyword">elif</span> self.x - <span class="hljs-number">1</span> == <span class="hljs-number">0</span>:<br>                score = -<span class="hljs-number">1</span><br>                pre_done = <span class="hljs-literal">True</span><br>            <span class="hljs-keyword">else</span>:<br>                score = <span class="hljs-number">0</span><br>                pre_done = <span class="hljs-literal">False</span><br>            <span class="hljs-keyword">return</span> self.<span class="hljs-built_in">map</span>[self.x - <span class="hljs-number">1</span>], score, pre_done<span class="hljs-comment"># 返回下一个状态、奖励和结束标志</span><br> <br>    <span class="hljs-comment"># 初始化，位置0，计数器归零</span><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">retry</span>(<span class="hljs-params">self, start_colum</span>):  <span class="hljs-comment"># 初始化</span><br>        self.x = start_colum<br>        self.count = <span class="hljs-number">0</span><br></code></pre></td></tr></table></figure><p>Agent.py</p><p>这是智能体的初始化、选择行动和学习的代码。  </p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">import</span> numpy <span class="hljs-keyword">as</span> np<br><span class="hljs-keyword">import</span> pandas <span class="hljs-keyword">as</span> pd<br> <br> <br><span class="hljs-keyword">class</span> <span class="hljs-title class_">Agent</span>:<br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">__init__</span>(<span class="hljs-params">self, actions, long, learning_rate=<span class="hljs-number">0.05</span>, reward_decay=<span class="hljs-number">0.9</span>, e_greedy=<span class="hljs-number">0.9</span></span>):<br>        self.actions = actions  <span class="hljs-comment"># 初始化可以进行的各种行为，传入为列表</span><br>        self.lr = learning_rate  <span class="hljs-comment"># 学习率，用于更新Q_table的值</span><br>        self.gamma = reward_decay  <span class="hljs-comment"># 当没有到达终点时，下一环境对当前环境的影响</span><br>        <span class="hljs-comment"># self.epsilon = e_greedy  # 随机选择几率为1-e_greedy，当处于e_greedy内时，不随机选择。</span><br>        self.q_table = pd.DataFrame(columns=self.actions, dtype=np.float64)  <span class="hljs-comment"># 生成q_table，列向量为columns</span><br>        <span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(long):<br>            line_table = pd.Series(<br>                [<span class="hljs-number">0.0</span>, <span class="hljs-number">0.0</span>],<br>                name=i,<br>                index=actions<br>            )<br>            line_table_2_frame = line_table.to_frame()<br>            self.q_table = pd.concat([self.q_table, line_table_2_frame.T])<br> <br>    <span class="hljs-comment"># 选择行动</span><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">choose_action</span>(<span class="hljs-params">self, observation</span>):<br>        action_list = self.q_table.loc[observation, :]  <span class="hljs-comment"># 取出当前observation所在的不同方向</span><br> <br>        <span class="hljs-comment"># if np.random.uniform() &lt; self.epsilon:  # 如果在epsilon几率内</span><br>        <span class="hljs-comment">#     # 选出当前observation中Q值最大的方向，这里再加一个random.choice是为了防止出现两个概率相同</span><br>        <span class="hljs-comment">#     action = np.random.choice(action_list[action_list == np.max(action_list)].index)</span><br>        <span class="hljs-comment"># else:</span><br>        <span class="hljs-comment">#     action = np.random.choice(self.actions)  # 如果不在epsilon内，则随机选择一个动作</span><br> <br>        action = np.random.choice(action_list[action_list == np.<span class="hljs-built_in">max</span>(action_list)].index)  <span class="hljs-comment"># action总选择最优解</span><br>        <span class="hljs-keyword">return</span> action  <span class="hljs-comment"># 返回应当做的action</span><br> <br>    <span class="hljs-comment"># 学习</span><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">learn</span>(<span class="hljs-params">self, observation_now, action, score, observation_after, done</span>):<br>        q_predict = self.q_table.loc[observation_now, action]  <span class="hljs-comment"># 获得当前状态下，当前所作动作所对应的预测得分</span><br>        <span class="hljs-keyword">if</span> done:<br>            q_target = score  <span class="hljs-comment"># 如果完成了则q_target为下一个环境的实际情况得分，本例子中此时score为1</span><br>        <span class="hljs-keyword">else</span>:<br>            <span class="hljs-comment"># 如果未完成则取下一个环境若干个动作中的最大得分作为这个环境的价值传递给当前环境</span><br>            q_target = score + self.gamma * self.q_table.loc[observation_after, :].<span class="hljs-built_in">max</span>()<br> <br>        <span class="hljs-comment"># 根据所处的当前环境对各个动作的预测得分和下一步的环境的实际情况更新当前环境的q表</span><br>        self.q_table.loc[observation_now, action] += self.lr * (q_target - q_predict)<br></code></pre></td></tr></table></figure><p>此处的选择行动，可以设置一定的随机率，比如说设置为0.9的几率为选择Q-table中最优的action，剩下0.1的概率让agent随机选择。或者是不设置随机率，每一步都贪心。</p><p>接下来是main</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">from</span> Env <span class="hljs-keyword">import</span> Env<br><span class="hljs-keyword">from</span> Agent <span class="hljs-keyword">import</span> Agent<br><span class="hljs-keyword">import</span> time<br><span class="hljs-keyword">import</span> pandas<br><br><span class="hljs-keyword">import</span> os<br> <br>LONG = <span class="hljs-number">7</span>  <span class="hljs-comment"># 总长度为6</span><br>START_PLACE = <span class="hljs-number">1</span>  <span class="hljs-comment"># 游戏开始的位置</span><br>MAZE_PLACE = <span class="hljs-number">7</span>  <span class="hljs-comment"># 宝藏在第六位</span><br>TIMES = <span class="hljs-number">1000</span>  <span class="hljs-comment"># 限制最大1000次循环，防止死循环</span><br>STOP_FLAG = <span class="hljs-literal">False</span><br>e = <span class="hljs-number">1e-2</span><br> <br>people = Agent([<span class="hljs-string">&#x27;left&#x27;</span>, <span class="hljs-string">&#x27;right&#x27;</span>], LONG)  <span class="hljs-comment"># 生成QLearn主体的对象，包含left和right，传入两个action</span><br>site = Env(LONG, START_PLACE, MAZE_PLACE)  <span class="hljs-comment"># 生成测试环境</span><br><span class="hljs-keyword">for</span> episode <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(TIMES):<br>    state = site.get_observation()  <span class="hljs-comment"># 观察初始环境</span><br>    site.draw()  <span class="hljs-comment"># 生成图像</span><br>    time.sleep(<span class="hljs-number">0.2</span>)  <span class="hljs-comment"># 暂停</span><br>    <span class="hljs-keyword">while</span> <span class="hljs-literal">True</span>:<br>        done = site.get_terminal()  <span class="hljs-comment"># 判断当前环境是否到达最后</span><br>        <span class="hljs-keyword">if</span> done:  <span class="hljs-comment"># 如果到达，则初始化</span><br>            interaction = <span class="hljs-string">&#x27;\n第%s次episode，共使用步数：%s。&#x27;</span> % (episode + <span class="hljs-number">1</span>, site.count)<br>            <span class="hljs-built_in">print</span>(interaction)<br>            <span class="hljs-comment"># 存储本次记录，计算与上次最大差值</span><br>            <span class="hljs-keyword">if</span> <span class="hljs-keyword">not</span> os.path.exists(<span class="hljs-string">&#x27;data&#x27;</span>):<br>                os.makedirs(<span class="hljs-string">&#x27;data&#x27;</span>)<br>            fileName = <span class="hljs-string">&quot;data/episode&quot;</span> + <span class="hljs-built_in">str</span>(episode) + <span class="hljs-string">&quot;.csv&quot;</span><br>            people.q_table.to_csv(fileName)  <span class="hljs-comment"># 将本次的q_table存储到本地文件中</span><br>            <span class="hljs-comment"># print(f&quot;\n第&#123;episode&#125;轮数据：\n&#123;people.q_table&#125;\n&quot;)</span><br>            <span class="hljs-keyword">if</span> episode != <span class="hljs-number">0</span>:  <span class="hljs-comment"># 第一轮不进行判断</span><br>                old_file_name = <span class="hljs-string">&quot;data/episode&quot;</span> + <span class="hljs-built_in">str</span>(episode - <span class="hljs-number">1</span>) + <span class="hljs-string">&quot;.csv&quot;</span>  <span class="hljs-comment"># 读取上一次的q_table</span><br>                old_q_table = pandas.read_csv(old_file_name, index_col=<span class="hljs-number">0</span>)<br>                <span class="hljs-comment"># print(f&quot;\n第&#123;episode - 1&#125;轮数据：\n&#123;old_q_table&#125;\n&quot;)</span><br>                difference = (people.q_table - old_q_table).<span class="hljs-built_in">abs</span>()<br>                <span class="hljs-comment"># print(f&quot;两次差值：\n&#123;difference&#125;\n&quot;)</span><br>                max_difference = difference.<span class="hljs-built_in">max</span>()[<span class="hljs-number">0</span>] \<br>                    <span class="hljs-keyword">if</span> difference.<span class="hljs-built_in">max</span>()[<span class="hljs-number">0</span>] &gt;= difference.<span class="hljs-built_in">max</span>()[<span class="hljs-number">1</span>] <span class="hljs-keyword">else</span> difference.<span class="hljs-built_in">max</span>()[<span class="hljs-number">1</span>]<br>                <span class="hljs-comment"># print(f&quot;与上一次最大差值：\n&#123;difference.max()&#125;\n&#123;difference.max()[0]&#125;,&#123;difference.max()[1]&#125;\n&quot;)</span><br>                <span class="hljs-built_in">print</span>(<span class="hljs-string">f&quot;最大差值：<span class="hljs-subst">&#123;max_difference&#125;</span>&quot;</span><br>                      <span class="hljs-string">f&quot;\n------<span class="hljs-subst">&#123;episode + <span class="hljs-number">1</span>&#125;</span>------&quot;</span>)<br>                <span class="hljs-keyword">if</span> max_difference &lt;= e:  <span class="hljs-comment"># 达到收敛条件</span><br>                    STOP_FLAG = <span class="hljs-literal">True</span><br>                    <span class="hljs-keyword">break</span><br>            site.retry(START_PLACE)  <span class="hljs-comment"># 初始化</span><br>            time.sleep(<span class="hljs-number">0.5</span>)<br>            <span class="hljs-keyword">break</span><br>        action = people.choose_action(state)  <span class="hljs-comment"># 获得下一步方向</span><br>        state_after, score, pre_done = site.get_target(action)  <span class="hljs-comment"># 获得下一步的环境的实际情况</span><br>        people.learn(state, action, score, state_after, pre_done)  <span class="hljs-comment"># 根据所处的当前环境对各个动作的预测得分和下一步的环境的实际情况更新当前环境的q表</span><br>        site.update_place(action)  <span class="hljs-comment"># 更新位置</span><br>        state = state_after  <span class="hljs-comment"># 状态更新</span><br>        site.draw()  <span class="hljs-comment"># 更新画布</span><br>        time.sleep(<span class="hljs-number">0.2</span>)<br>    <span class="hljs-keyword">if</span> STOP_FLAG:<br>        <span class="hljs-keyword">break</span><br> <br><span class="hljs-built_in">print</span>(people.q_table)<br></code></pre></td></tr></table></figure><p>这里可以设置两种结束条件，一种是比较简单的设置循环次数，比如说循环20次、50次就强制结束，此种方法的缺点是，不能保证此结果已经收敛；还有一种方法是，加上收敛的判定条件，比如以上代码设置了判断收敛的方法，即判断本次Q-table的和上一次Q-table最大的差值，是否小于设定的一个阙值，若小于此阙值，则已经收敛，若不小于，则继续迭代。</p>]]></content>
    
    
    <categories>
      
      <category>强化学习</category>
      
    </categories>
    
    
    <tags>
      
      <tag>强化学习</tag>
      
      <tag>Q-learning</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Q table</title>
    <link href="/2024/05/13/Q-table/"/>
    <url>/2024/05/13/Q-table/</url>
    
    <content type="html"><![CDATA[<h1 id="Q-table"><a href="#Q-table" class="headerlink" title="Q table"></a>Q table</h1><p>要训练的就是Q table  </p><img src="/2024/05/13/Q-table/Qtable1.png" class="" title="Qtable1">  <img src="/2024/05/13/Q-table/Qtable2.png" class="" title="Qtable2">  ]]></content>
    
    
    <categories>
      
      <category>强化学习</category>
      
    </categories>
    
    
    <tags>
      
      <tag>强化学习</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>SARSA算法和Q-learning算法</title>
    <link href="/2024/05/12/SARSA%E7%AE%97%E6%B3%95%E5%92%8CQ-learning%E7%AE%97%E6%B3%95/"/>
    <url>/2024/05/12/SARSA%E7%AE%97%E6%B3%95%E5%92%8CQ-learning%E7%AE%97%E6%B3%95/</url>
    
    <content type="html"><![CDATA[<h1 id="SARSA-1994"><a href="#SARSA-1994" class="headerlink" title="SARSA(1994)"></a>SARSA(1994)</h1><p>之前学习了用 TD 估算 V值。但其实我们用 TD 预估 Q 值，其实会来得更方便，因为我们要的就是智能体选择动作嘛。智能体是不是可以看成是个Q function? Q function的输入是 St和 At，这样在t时刻 St状态下，选择最好的 action At，不就能指导智能体了吗?  </p><p>SARSA 的想法是，用同一个策略下产生的动作A的 Q值替代 V(St+1)。  </p><img src="/2024/05/12/SARSA%E7%AE%97%E6%B3%95%E5%92%8CQ-learning%E7%AE%97%E6%B3%95/SARSA.png" class="" title="SARSA">  <p>TD 估算 V 值对比一下，几乎是一模一样的，只是把V换成 Q。  </p><h1 id="Q-learning-1989"><a href="#Q-learning-1989" class="headerlink" title="Q-learning(1989)"></a>Q-learning(1989)</h1><p>Q learning 的想法其实也很直观:既然我们的目标是选取最大收益，所以，我们肯定会选择一个能够获得最大 Q值的动作。也就是说，在实际选择中，我不可能选择不是最大Q 值的动作。所以，我们应该用所有动作的 Q 值的最大值替代 V(St+1)。  </p><img src="/2024/05/12/SARSA%E7%AE%97%E6%B3%95%E5%92%8CQ-learning%E7%AE%97%E6%B3%95/Qlearning.png" class="" title="Qlearning">  <p>Q learning 公式和 SARSA 相比，就差一个 max。</p>]]></content>
    
    
    <categories>
      
      <category>强化学习</category>
      
    </categories>
    
    
    <tags>
      
      <tag>强化学习</tag>
      
      <tag>Q-learning</tag>
      
      <tag>SARSA</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>蒙特卡洛和时序差分估算状态V值</title>
    <link href="/2024/05/12/%E8%92%99%E7%89%B9%E5%8D%A1%E6%B4%9B%E5%92%8C%E6%97%B6%E5%BA%8F%E5%B7%AE%E5%88%86%E4%BC%B0%E7%AE%97%E7%8A%B6%E6%80%81V%E5%80%BC/"/>
    <url>/2024/05/12/%E8%92%99%E7%89%B9%E5%8D%A1%E6%B4%9B%E5%92%8C%E6%97%B6%E5%BA%8F%E5%B7%AE%E5%88%86%E4%BC%B0%E7%AE%97%E7%8A%B6%E6%80%81V%E5%80%BC/</url>
    
    <content type="html"><![CDATA[<h1 id="Monte-Carlo-估算状态V值"><a href="#Monte-Carlo-估算状态V值" class="headerlink" title="Monte Carlo 估算状态V值"></a>Monte Carlo 估算状态V值</h1><p>为了方便，我们对平均进行一些优化。于是获得用 MC估算V值的公式:  </p><p>首先明确增量更新法，它有点像梯度下降法，GBDT残差不就是负梯度  </p><ul><li>新平均&#x3D;旧日平均 + 步长*(新加入元素-旧日平均)</li></ul><p>那么每次更新我们就能写出如下式子:  </p><img src="/2024/05/12/%E8%92%99%E7%89%B9%E5%8D%A1%E6%B4%9B%E5%92%8C%E6%97%B6%E5%BA%8F%E5%B7%AE%E5%88%86%E4%BC%B0%E7%AE%97%E7%8A%B6%E6%80%81V%E5%80%BC/%E6%97%B6%E5%BA%8F%E5%B7%AE%E5%88%86%E5%9B%BE1.png" class="" title="时序差分图1">  <h1 id="TD估算状态V值-1959"><a href="#TD估算状态V值-1959" class="headerlink" title="TD估算状态V值 (1959)"></a>TD估算状态V值 (1959)</h1><p>时序差分(temporal-difference )方法又称 TD 方法是强化学习中应用最为广泛的一种学习方法。  </p><p>TD 算法对蒙地卡罗(MC)进行了改进  </p><ol><li>和蒙地卡罗(MC)不同:TD 算法只需要走 N 步。就可以开始回溯更新。  </li><li>和蒙地卡罗(MC)一样:需要先走 N步，每经过一个状态，把奖励记录下来。然后开始回溯。</li><li>那么，状态的 V 值怎么算呢?其实和蒙地卡罗一样，我们就假设N步之后，就到达了最终状态了。-假设“最终状态”上我们之前没有走过，所以这个状态上的纸是空白的。这个时候我们就当这个状态为 0.-假设“最终状态”上我们已经走过了,这个状态的V值就是当前值。然后我们开始回溯。</li></ol><p>我们可以把 TD 看成是这样一种情况:  </p><img src="/2024/05/12/%E8%92%99%E7%89%B9%E5%8D%A1%E6%B4%9B%E5%92%8C%E6%97%B6%E5%BA%8F%E5%B7%AE%E5%88%86%E4%BC%B0%E7%AE%97%E7%8A%B6%E6%80%81V%E5%80%BC/%E6%97%B6%E5%BA%8F%E5%B7%AE%E5%88%86%E5%9B%BE2.png" class="" title="时序差分图2">  <p>经过 1步，到 B状态。我们什么都不管就当B状态是最终状态了。  </p><p>我们从 A 状态，但 B状态本身就带有一定的价值，也就是V值。其意义就是从 B状态到最终状态的总价值期望。我们假设 B状态的V值是对的，那么，通过回溯计算，我们就能知道 A 状态的更新目标了。  </p><p>于是获得用 TD 估算 V值的公式：  </p><p>在 MC，G 是更新目标，而在 TD，我们只不过把更新目标从 G，改成r+gamma*V</p><img src="/2024/05/12/%E8%92%99%E7%89%B9%E5%8D%A1%E6%B4%9B%E5%92%8C%E6%97%B6%E5%BA%8F%E5%B7%AE%E5%88%86%E4%BC%B0%E7%AE%97%E7%8A%B6%E6%80%81V%E5%80%BC/%E6%97%B6%E5%BA%8F%E5%B7%AE%E5%88%86%E5%9B%BE3.png" class="" title="时序差分图3">  ]]></content>
    
    
    <categories>
      
      <category>强化学习</category>
      
    </categories>
    
    
    <tags>
      
      <tag>强化学习</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>蒙特卡洛采样回溯计算V值</title>
    <link href="/2024/05/11/%E8%92%99%E7%89%B9%E5%8D%A1%E6%B4%9B%E9%87%87%E6%A0%B7%E5%9B%9E%E6%BA%AF%E8%AE%A1%E7%AE%97V%E5%80%BC/"/>
    <url>/2024/05/11/%E8%92%99%E7%89%B9%E5%8D%A1%E6%B4%9B%E9%87%87%E6%A0%B7%E5%9B%9E%E6%BA%AF%E8%AE%A1%E7%AE%97V%E5%80%BC/</url>
    
    <content type="html"><![CDATA[<h1 id="Monte-Carlo-Sampling-1947"><a href="#Monte-Carlo-Sampling-1947" class="headerlink" title="Monte Carlo Sampling (1947)"></a>Monte Carlo Sampling (1947)</h1><ol><li>我们把智能体放到环境的任意状态</li><li>从这个状态开始按照策略进行选择动作，并进入新的状态。</li><li>重复步骤 2，直到最终状态</li><li>我们从最终状态开始向前回溯:计算每个状态的 G 值。</li><li>重复 1-4 多次，然后平均每个状态的 G 值，这就是我们需要求的 V 值。<img src="/2024/05/11/%E8%92%99%E7%89%B9%E5%8D%A1%E6%B4%9B%E9%87%87%E6%A0%B7%E5%9B%9E%E6%BA%AF%E8%AE%A1%E7%AE%97V%E5%80%BC/%E8%92%99%E7%89%B9%E5%8D%A1%E6%B4%9B%E9%87%87%E6%A0%B7%E5%9B%9E%E6%BA%AF.png" class="" title="蒙特卡洛采样回溯"></li></ol><p>第一步，我们根据策略往前走，一直走到最后，期间我们什么都不用算，还需要记录每个状态转移，我们获得多少奖励r即可。</p><p>第二步，我们从终点往前走，一遍走一遍计算 G 值。G 值等于上一个状态的 G 值(记作G’),乘以一定的折扣(gamma),再加上r。  </p><blockquote><p><strong>折扣率</strong> 在强化学习中，有某些参数是人为主观制定不能推导，但在实际应用中却能解决问题，所以我们称这些参数为超参数，而折扣率就是一个超参数。与金融产品说的贴现率是类似的。我们计算价值，目的就是把未来很多步奖励，折算到当前节点。但未来n步的奖励的 10 点奖励，与当前的 10 点奖励是否完全等价呢?未必。所以我们人为地给未来的奖励一定的折扣，例如:0.9,0.8，然后在计算到当前的价值。  </p></blockquote><p>所以G值的意义在于，在一次游戏中，某个状态到最终状态的奖励总和(简单理解时科研忽略折扣值)  </p><img src="/2024/05/11/%E8%92%99%E7%89%B9%E5%8D%A1%E6%B4%9B%E9%87%87%E6%A0%B7%E5%9B%9E%E6%BA%AF%E8%AE%A1%E7%AE%97V%E5%80%BC/%E8%92%99%E7%89%B9%E5%8D%A1%E6%B4%9B%E9%87%87%E6%A0%B7%E5%9B%9E%E6%BA%AF2.png" class="" title="蒙特卡洛采样回溯2">  <p>当我们进行多次试验后，我们有可能会经过某个状态多次，通过回，也会有多个G.重复我们刚才说的，每一个G值，就是每次到最终状态获得的奖励总和。而V值，某个状态下，我们通过影分身到达最终状态，所有影分身获得的奖励的平均值。  </p><img src="/2024/05/11/%E8%92%99%E7%89%B9%E5%8D%A1%E6%B4%9B%E9%87%87%E6%A0%B7%E5%9B%9E%E6%BA%AF%E8%AE%A1%E7%AE%97V%E5%80%BC/%E8%92%99%E7%89%B9%E5%8D%A1%E6%B4%9B%E9%87%87%E6%A0%B7%E5%9B%9E%E6%BA%AF3.png" class="" title="蒙特卡洛采样回溯3">  <ol><li>G 的意义:在某个路径上，状态S到最终状态的总收获。</li><li>V和 G 的关系:V 是 G的平均数。</li></ol><p>到这里要注意一点:V和策略是相关的，那么在这里怎么体现呢?  </p><img src="/2024/05/11/%E8%92%99%E7%89%B9%E5%8D%A1%E6%B4%9B%E9%87%87%E6%A0%B7%E5%9B%9E%E6%BA%AF%E8%AE%A1%E7%AE%97V%E5%80%BC/%E8%92%99%E7%89%B9%E5%8D%A1%E6%B4%9B%E9%87%87%E6%A0%B7%E5%9B%9E%E6%BA%AF4.png" class="" title="蒙特卡洛采样回溯4">  <p>蒙地卡罗有一个比较大的缺点，就是每一次游戏，都需要先从头走到尾，再进行回溯更新。如果最终状态很难达到，可能每一次都要转很久很久才能更新一次 G 值。</p>]]></content>
    
    
    <categories>
      
      <category>强化学习</category>
      
    </categories>
    
    
    <tags>
      
      <tag>强化学习</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>马尔科夫链</title>
    <link href="/2024/05/11/%E9%A9%AC%E5%B0%94%E7%A7%91%E5%A4%AB%E9%93%BE/"/>
    <url>/2024/05/11/%E9%A9%AC%E5%B0%94%E7%A7%91%E5%A4%AB%E9%93%BE/</url>
    
    <content type="html"><![CDATA[<h1 id="马尔可夫链"><a href="#马尔可夫链" class="headerlink" title="马尔可夫链"></a>马尔可夫链</h1><p>马尔可夫博弈（Markov Game）也被称为随机博弈。  </p><img src="/2024/05/11/%E9%A9%AC%E5%B0%94%E7%A7%91%E5%A4%AB%E9%93%BE/%E9%A9%AC%E5%B0%94%E7%A7%91%E5%A4%AB%E9%93%BE.png" class="" title="马尔科夫链">  <p>有三个重要的元素:S，A，R。我们分别来看一下，他们代表的是什么。然后大家就会明白，为什么马尔科夫链是一个很好很常用的模型。  </p><p>1.智能体在环境中，观察到状态(S)  </p><p>2.状态(S)被输入到智能体，智能体经过计算，选择动作(A);  </p><p>3.动作(A)使智能体进入另外一个状态(S)，并返回奖励(R)给智能体,重复以上步骤，一步一步创造马尔科夫链。  </p><p>4.智能体根据返回，调整自己的策略。  </p><p>所以你看，强化学习跟教孩子是一个道理: 孩子做了好事，必须给奖励;孩子做错事了必须惩罚。就这么简单!  </p><p>其中的不确定性:  </p><ol><li><p>“选择”的过程。<br>智能体“选择”会影响到下一个状态。比如state&#x2F;observation 一样，agent 对于 action 的选择也可能不同，这种不同动作之间的选择，我们称为智能体的策略。策略我们一般用Π表示。我们的任务就是找到一个策略，能够获得最多的奖励。  </p></li><li><p>环境的随机性。<br>这是智能体无法控制的，比如 action 一样但反馈回来新的 state&#x2F;observation 或reward 也可能有所不同。但马尔科夫链允许我们有不确定性的存在。</p></li></ol><p>所以，这种不确定性来自两个方面:1.智能体的行动选择(策略)。2.环境的不确定性。</p><h1 id="Q值和V值"><a href="#Q值和V值" class="headerlink" title="Q值和V值"></a>Q值和V值</h1><p>当智能体从一个状态S，选择动作 A，会进入另外一个状态 S’同时，也会给智能体奖励 R。奖励既有正，也有负。正代表我们鼓励智能体在这个状态下继续这么做;负得话代表我们并不希望智能体这么做。在强化学习中，我们会用奖励 R作为智能体学习的引导，期望智能体获得尽可能多的奖励。  </p><p>但更多的时候，我们并不能单纯通过 R来衡量一个动作的好坏。我们必须用长远的眼光来看待问题。我们要把<strong>未来的奖励</strong>也计算到当前状态下，再进行决策。  </p><p>举例，就好比下棋，前期布局当中某一步的效果reward，并不能直接反应出它的作用，再如中局弃子，可能会使得当前reward 为负，但是或许可以带来未来更大的胜利!  </p><img src="/2024/05/11/%E9%A9%AC%E5%B0%94%E7%A7%91%E5%A4%AB%E9%93%BE/alphago.png" class="" title="alphago">  <p>所以我们在做决策的时候，需要把眼光放远点，把未来的价值换到当前，才能做出选择。  </p><p>为了方便，我们希望可以有一种方法衡量 agent 做出每种选择的价值。这样，我如果通过方法未卜先知，以后的事情我也不用理了，agent 当前就能选择哪个动作价值更大，就选那个动作就可以了。</p><img src="/2024/05/11/%E9%A9%AC%E5%B0%94%E7%A7%91%E5%A4%AB%E9%93%BE/%E9%A9%AC%E5%B0%94%E7%A7%91%E5%A4%AB%E9%93%BE.png" class="" title="马尔科夫链">  <p><strong>评伦动作的价值，我们称为Q值:</strong> 它代表了智能体选择这个动作后，一直到最终状态奖励总和的期望;  </p><p><strong>评估状态的价值，我们称为V值:</strong> 它代表了智能体在这个状态下，一直到最终状态的奖励总和的期望。  </p><p>价值越高，表示我从当前状态到最终状态能获得的平均奖励将会越高。因为智能体的目标数是获取尽可能多的奖励，所以智能体在当前状态，只需要选择价值高的动作就可以了。</p><h2 id="V值"><a href="#V值" class="headerlink" title="V值"></a>V值</h2><p>假设现在需要求某个状态S的V值，那么我们可以这样：  </p><img src="/2024/05/11/%E9%A9%AC%E5%B0%94%E7%A7%91%E5%A4%AB%E9%93%BE/%E5%BD%B1%E5%88%86%E8%BA%AB.png" class="" title="影分身">  <ol><li>我们从S点出发，并影分身出若干个自己;  </li><li>每个分身按照当前的策略 选择行为;</li><li>每个分身一直走到最终状态，并计算一路上获得的所有奖励总和;</li><li>我们计算每个影分身获得的平均值,这个平均值就是我们要求的 V值。</li></ol><p>用大白话总结就是:从某个状态，按照策略走到最终，最终获得奖励很多很多次，总和的平均值，就是V值。  </p><p>V 值是会根据不同的策略有所变化的!  </p><p>现在我们假设策略 采用平均策略[A1:50%,A2:50%],根据用影分身(如果是学霸直接求期望)，那么我们可以求得 V 值为 15  </p><img src="/2024/05/11/%E9%A9%AC%E5%B0%94%E7%A7%91%E5%A4%AB%E9%93%BE/%E5%BD%B1%E5%88%86%E8%BA%AB%E8%A1%A81.png" class="" title="影分身表1">  <p>现在我们改变策略[A1:60%,A2:40%]，那么我们可以求得 V值为 14，变少了!</p><img src="/2024/05/11/%E9%A9%AC%E5%B0%94%E7%A7%91%E5%A4%AB%E9%93%BE/%E5%BD%B1%E5%88%86%E8%BA%AB%E8%A1%A82.png" class="" title="影分身表2">  <h2 id="Q值"><a href="#Q值" class="headerlink" title="Q值"></a>Q值</h2><p>Q 值和 V值的概念是一致的，都是衡量在马可洛夫树上某一个节点的价值。只不过V值衡量的是状态节点的价值，而 Q值衡量的是动作节点的价值。<br>现在我们需要计算，某个状态 S0 下的一个动作 A的 Q 值:<br>1.我们就可以从 A 这个节点出发，使用影分身之术;<br>2.每个影分身走到最终状态,并记录所获得的奖励;<br>3.求取所有影分身获得奖励的平均值，这个平均值就是我们需要求的 Q 值。<br>用大白话总结就是:从某个状态选取动作 A，走到最终状态很多很多次;最终获得奖励总和的平均值，就是 Q值。  </p><ul><li>与V值不同，Q 值和策略并没有直接相关，而与环境的状态转移概率相关而环境的状态转移概率是不变的。</li></ul><h2 id="V值和Q值的关系"><a href="#V值和Q值的关系" class="headerlink" title="V值和Q值的关系"></a>V值和Q值的关系</h2><p>我们可以知道 Q 值和 V 值的意义相通的:</p><ol><li>都是马可洛夫树上的节点</li><li>价值评价的方式是一样的:<br> 从当前节点出发 - 一直走到最终节点 - 所有的奖励的期望值</li></ol><p>V 就是子节点的 Q的期望!但要注意 V值和策略相关。<br>Q 就是子节点的 V的期望!但要注意，记得把R计算在内。</p>]]></content>
    
    
    <categories>
      
      <category>强化学习</category>
      
    </categories>
    
    
    <tags>
      
      <tag>强化学习</tag>
      
      <tag>马尔科夫链</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>强化学习基础</title>
    <link href="/2024/05/11/%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0%E5%9F%BA%E7%A1%80/"/>
    <url>/2024/05/11/%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0%E5%9F%BA%E7%A1%80/</url>
    
    <content type="html"><![CDATA[<h1 id="强化学习基础"><a href="#强化学习基础" class="headerlink" title="强化学习基础"></a>强化学习基础</h1><h2 id="强化学习的任务"><a href="#强化学习的任务" class="headerlink" title="强化学习的任务"></a>强化学习的任务</h2><p>希望用强化学习的方式，使每个东西获得独立自主地完成某种任务的能力，这个东西被称为智能体。而智能体学习和工作的地方，称为环境。  </p><ul><li>所谓独立自主，就是智能体一旦启动，就不需要人指挥</li></ul><p>例如：扫地机器人自动清理，自动驾驶汽车自动驾驶。</p><p>智能体与环境的交互如图：  </p><img src="/2024/05/11/%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0%E5%9F%BA%E7%A1%80/%E6%80%8E%E4%B9%88%E5%AD%A6%E4%B9%A0.png" class="" title="智能体与环境的交互">  <h2 id="A-actiopn-动作"><a href="#A-actiopn-动作" class="headerlink" title="A(actiopn)动作"></a>A(actiopn)动作</h2><p>动作就是智能体做出的具体行为。例如扫地机器人的移动，吸尘，喷水。无人驾驶汽车能够移动，加速，刹车。通信系统选择信道，选择调制编码方案等。  </p><p>动作空间就是该智能体能够做出的动作数量。<br>例如：智能体身处十字路口，方向就有四个，也就是动作有四个。</p><h2 id="R-reward-奖励"><a href="#R-reward-奖励" class="headerlink" title="R(reward)奖励"></a>R(reward)奖励</h2><p>当误码在某个状态下，完成动作。环境就会给我们<strong>反馈</strong>，告诉误码这个动作的效果如何。这种效果的数值表达，就是奖励。</p><p>其实这里的 reward 翻译为“反馈”可能更合适一点。因为反馈并不是完全正面的，也有负面。当奖励可以是正数，表示鼓励当前的行为;如果是负数负数，表示惩罚这种行为。当然也可以是 0。而奖励值的大小，表示鼓励的和惩罚的力度不同。  </p><p>奖励在强化学习中，起到了很关键的作用，我们会以奖励作为引导让智能体学习做能获得最多奖励的动作。  </p><p>例如:我需要训练机器人打乒乓球。机器人每次赢球，都可以加分;输球，就减分。这分数就表现了机器人的动作好坏。如果机器人希望获得更多的分数，就需要想办法赢球。  </p><p>又例如:无人驾驶汽车如果成功到达目标地点，那么可以获得奖励;但如果闯红灯，那么就会被扣除大量的奖励作为惩罚。如果无人驾驶汽车希望获得更多的分数，那么就必须在遵守交通规则的情况下，成功到达目标地点。  </p><p>注意，奖励的设定是主观的，也就是说我们为了智能体更好地学习工作，自己定的。所以大家可以看到，很多时候我们会对奖励进行一定的修正，这也是加速智能体学习的方法之一。  </p><p><strong>在通信系统中</strong>每选择一个信道或者调制编码方案，都会获得不同的信干噪比或者是误码率，表示了通信效能的好坏。  </p>]]></content>
    
    
    <categories>
      
      <category>强化学习</category>
      
    </categories>
    
    
    <tags>
      
      <tag>强化学习</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>2.C-运算符</title>
    <link href="/2024/04/26/2.C-%E8%BF%90%E7%AE%97%E7%AC%A6/"/>
    <url>/2024/04/26/2.C-%E8%BF%90%E7%AE%97%E7%AC%A6/</url>
    
    <content type="html"><![CDATA[<h1 id="2-运算符"><a href="#2-运算符" class="headerlink" title="2. 运算符"></a>2. 运算符</h1><p><strong>作用：</strong> 用于执行代码的运算  </p><table><thead><tr><th>运算符类型</th><th>作用</th></tr></thead><tbody><tr><td>算术运算符</td><td>用于处理四则运算</td></tr><tr><td>赋值运算符</td><td>用于将表达式的值赋给变量</td></tr><tr><td>比较运算符</td><td>用于表达式的比较，并返回一个真值或假值</td></tr><tr><td>逻辑运算符</td><td>用于根据表达式的值返回真值或假值</td></tr></tbody></table><h2 id="2-1-算术运算符"><a href="#2-1-算术运算符" class="headerlink" title="2.1 算术运算符"></a>2.1 算术运算符</h2><p><strong>作用：</strong> 用于处理四则运算</p><p>算术运算符包括以下符号：</p><table><thead><tr><th>运算符</th><th>术语</th><th>示例</th><th>结果</th></tr></thead><tbody><tr><td>+</td><td>正号</td><td>+3</td><td>3</td></tr><tr><td>-</td><td>负号</td><td>-3</td><td>-3</td></tr><tr><td>+</td><td>加</td><td>10 + 5</td><td>15</td></tr><tr><td>-</td><td>减</td><td>10- 5</td><td>5</td></tr><tr><td>*</td><td>乘</td><td>10 * 5</td><td>50</td></tr><tr><td>&#x2F;</td><td>除</td><td>10 &#x2F; 5</td><td>2</td></tr><tr><td>%</td><td>取模(取余)</td><td>10 % 3</td><td>1</td></tr><tr><td>++</td><td>前置递增</td><td>a&#x3D;2; b&#x3D;++a;</td><td>a&#x3D;3; b&#x3D;3;</td></tr><tr><td>++</td><td>后置递增</td><td>a&#x3D;2; b&#x3D;a++;</td><td>a&#x3D;3; b&#x3D;2;</td></tr><tr><td>–</td><td>前置递减</td><td>a&#x3D;2; b&#x3D;–a;</td><td>a&#x3D;1; b&#x3D;1;</td></tr><tr><td>–</td><td>后置递减</td><td>a&#x3D;2; b&#x3D;a–;</td><td>a&#x3D;1; b&#x3D;2;</td></tr></tbody></table><p><strong>示例1：</strong>  </p><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br></pre></td><td class="code"><pre><code class="hljs C++"><span class="hljs-comment">//加减乘除</span><br><span class="hljs-function"><span class="hljs-type">int</span> <span class="hljs-title">main</span><span class="hljs-params">()</span> </span>&#123;<br><br>    <span class="hljs-type">int</span> a1 = <span class="hljs-number">10</span>;<br>    <span class="hljs-type">int</span> b1 = <span class="hljs-number">3</span>;<br><br>    cout &lt;&lt; a1 + b1 &lt;&lt; endl;<br>    cout &lt;&lt; a1 - b1 &lt;&lt; endl;<br>    cout &lt;&lt; a1 * b1 &lt;&lt; endl;<br>    cout &lt;&lt; a1 / b1 &lt;&lt; endl;  <span class="hljs-comment">//两个整数相除结果依然是整数</span><br><br>    <span class="hljs-type">int</span> a2 = <span class="hljs-number">10</span>;<br>    <span class="hljs-type">int</span> b2 = <span class="hljs-number">20</span>;<br>    cout &lt;&lt; a2 / b2 &lt;&lt; endl; <br><br>    <span class="hljs-type">int</span> a3 = <span class="hljs-number">10</span>;<br>    <span class="hljs-type">int</span> b3 = <span class="hljs-number">0</span>;<br>    <span class="hljs-comment">//cout &lt;&lt; a3 / b3 &lt;&lt; endl; //报错，除数不可以为0</span><br><br><br>    <span class="hljs-comment">//两个小数可以相除</span><br>    <span class="hljs-type">double</span> d1 = <span class="hljs-number">0.5</span>;<br>    <span class="hljs-type">double</span> d2 = <span class="hljs-number">0.25</span>;<br>    cout &lt;&lt; d1 / d2 &lt;&lt; endl;<br><br>    <span class="hljs-built_in">system</span>(<span class="hljs-string">&quot;pause&quot;</span>);<br><br>    <span class="hljs-keyword">return</span> <span class="hljs-number">0</span>;<br>&#125;<br></code></pre></td></tr></table></figure><blockquote><p>注意：在除法运算中，除数不能为0  </p></blockquote><p><strong>示例2：</strong>  </p><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br></pre></td><td class="code"><pre><code class="hljs C++"><span class="hljs-comment">//取模</span><br><span class="hljs-function"><span class="hljs-type">int</span> <span class="hljs-title">main</span><span class="hljs-params">()</span> </span>&#123;<br><br>    <span class="hljs-type">int</span> a1 = <span class="hljs-number">10</span>;<br>    <span class="hljs-type">int</span> b1 = <span class="hljs-number">3</span>;<br><br>    cout &lt;&lt; <span class="hljs-number">10</span> % <span class="hljs-number">3</span> &lt;&lt; endl;<br><br>    <span class="hljs-type">int</span> a2 = <span class="hljs-number">10</span>;<br>    <span class="hljs-type">int</span> b2 = <span class="hljs-number">20</span>;<br><br>    cout &lt;&lt; a2 % b2 &lt;&lt; endl;<br><br>    <span class="hljs-type">int</span> a3 = <span class="hljs-number">10</span>;<br>    <span class="hljs-type">int</span> b3 = <span class="hljs-number">0</span>;<br><br>    <span class="hljs-comment">//cout &lt;&lt; a3 % b3 &lt;&lt; endl; //取模运算时，除数也不能为0，相除时除数不能为0</span><br><br>    <span class="hljs-comment">//两个小数不可以取模,两个小数可以相除</span><br>    <span class="hljs-type">double</span> d1 = <span class="hljs-number">3.14</span>;<br>    <span class="hljs-type">double</span> d2 = <span class="hljs-number">1.1</span>;<br><br>    <span class="hljs-comment">//cout &lt;&lt; d1 % d2 &lt;&lt; endl;</span><br><br>    <span class="hljs-built_in">system</span>(<span class="hljs-string">&quot;pause&quot;</span>);<br><br>    <span class="hljs-keyword">return</span> <span class="hljs-number">0</span>;<br>&#125;<br></code></pre></td></tr></table></figure><p><strong>示例3：</strong>  </p><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br></pre></td><td class="code"><pre><code class="hljs C++"><span class="hljs-comment">//递增</span><br><span class="hljs-function"><span class="hljs-type">int</span> <span class="hljs-title">main</span><span class="hljs-params">()</span> </span>&#123;<br><br>    <span class="hljs-comment">//后置递增</span><br>    <span class="hljs-type">int</span> a = <span class="hljs-number">10</span>;<br>    a++; <span class="hljs-comment">//等价于a = a + 1</span><br>    cout &lt;&lt; a &lt;&lt; endl; <span class="hljs-comment">// 11</span><br><br>    <span class="hljs-comment">//前置递增</span><br>    <span class="hljs-type">int</span> b = <span class="hljs-number">10</span>;<br>    ++b;<br>    cout &lt;&lt; b &lt;&lt; endl; <span class="hljs-comment">// 11</span><br><br>    <span class="hljs-comment">//区别</span><br>    <span class="hljs-comment">//前置递增先对变量进行++，再计算表达式</span><br>    <span class="hljs-type">int</span> a2 = <span class="hljs-number">10</span>;<br>    <span class="hljs-type">int</span> b2 = ++a2 * <span class="hljs-number">10</span>;<br>    cout &lt;&lt; b2 &lt;&lt; endl;<br><br>    <span class="hljs-comment">//后置递增先计算表达式，后对变量进行++</span><br>    <span class="hljs-type">int</span> a3 = <span class="hljs-number">10</span>;<br>    <span class="hljs-type">int</span> b3 = a3++ * <span class="hljs-number">10</span>;<br>    cout &lt;&lt; b3 &lt;&lt; endl;<br><br>    <span class="hljs-built_in">system</span>(<span class="hljs-string">&quot;pause&quot;</span>);<br><br>    <span class="hljs-keyword">return</span> <span class="hljs-number">0</span>;<br>&#125;<br><br></code></pre></td></tr></table></figure>]]></content>
    
    
    <categories>
      
      <category>C++笔记</category>
      
    </categories>
    
    
    <tags>
      
      <tag>C++</tag>
      
      <tag>运算符</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>通信抗干扰策略_从评估到决策</title>
    <link href="/2024/04/25/%E9%80%9A%E4%BF%A1%E6%8A%97%E5%B9%B2%E6%89%B0%E7%AD%96%E7%95%A5_%E4%BB%8E%E8%AF%84%E4%BC%B0%E5%88%B0%E5%86%B3%E7%AD%96/"/>
    <url>/2024/04/25/%E9%80%9A%E4%BF%A1%E6%8A%97%E5%B9%B2%E6%89%B0%E7%AD%96%E7%95%A5_%E4%BB%8E%E8%AF%84%E4%BC%B0%E5%88%B0%E5%86%B3%E7%AD%96/</url>
    
    <content type="html"><![CDATA[<h1 id="1-评估（多属性决策问题）"><a href="#1-评估（多属性决策问题）" class="headerlink" title="1.评估（多属性决策问题）"></a>1.评估（多属性决策问题）</h1><h2 id="1-1-多属性决策问题研究意义"><a href="#1-1-多属性决策问题研究意义" class="headerlink" title="1.1 多属性决策问题研究意义"></a>1.1 多属性决策问题研究意义</h2><blockquote><p>随着紧急社会的不断发展，人们面临的决策形式和决策环境变得越来越复杂，无论政府的财政决策，还是企业的战略决策，都关乎其未来的发展。在决策科学领域存在一个重要分支，即多属性决策(multiple attribute decision making,MADM)。对多属性决策是由专家或评价者基于多个属性评估值，从多个备选方案中选出最优方案，以供决策者提供参考的过程。<br>常用的信息处理方法主要由两大类:一是传统的决策评价方法，如TOPSIS法、层次分析法(analytic hierarchy process,AHP)、灰色关联方法、投影分析法、VIKOR(vise kriterijumska optimizacijai kompromisino resenje)方法、ELECTRE(elimination et choice translation reality)方法等；二是信息集成算子，如简单的算术加权平均算子、几何加权算子，带有功能性的幂平均算子、BM算子、Heronian平均算子(Heronian mean,HM)以及MSM算子等。</p><blockquote><p>Q阶序对模糊多属性决策理论和方法_刘培德、王鹏  </p></blockquote></blockquote><p>我们需要这些评估方法(多属性决策方法)的原因，是为了解决决策形式和决策环境越来越复杂的问题，换句话说，也就是评估的指标越来越复杂，无法通过简单的方式来对一样事物进行评价。<br>评估的流程可以抽象为：1.选取指标 2. 获取指标参数 3. 综合计算。三步。<br><strong>举例</strong>：软科中国大学排名是一个利用层次分析法进行评估的例子。<br>该问题中决策者为选择学校的学生或老师，评价对象为学校。由于学校是一个综合的对象，存在很多方面的指标，决策者无法通过简单的一个或几个指标来判断一个大学的好坏，所以需要层次分析法，对指标进行分层和分别赋权，最后得到一个评分(评估结果)，最终得到一个大学的排序提供给决策者参考。<br>该问题中建立的层次分析法如图所示：  </p><img src="/2024/04/25/%E9%80%9A%E4%BF%A1%E6%8A%97%E5%B9%B2%E6%89%B0%E7%AD%96%E7%95%A5_%E4%BB%8E%E8%AF%84%E4%BC%B0%E5%88%B0%E5%86%B3%E7%AD%96/%E8%BD%AF%E7%A7%91%E8%AF%84%E4%BB%B7%E4%BD%93%E7%B3%BB.png" class="" width="400" title="软科评价体系">  <p>得到的评估结果如图：</p><img src="/2024/04/25/%E9%80%9A%E4%BF%A1%E6%8A%97%E5%B9%B2%E6%89%B0%E7%AD%96%E7%95%A5_%E4%BB%8E%E8%AF%84%E4%BC%B0%E5%88%B0%E5%86%B3%E7%AD%96/%E8%BD%AF%E7%A7%91%E8%AF%84%E4%BC%B0%E7%BB%93%E6%9E%9C.png" class="px" width="400" title="软科评估结果"> <blockquote><p>软科 2024软科中国大学排名  </p></blockquote><p>我们可以看出由于一个大学的评价存在多方面的多个指标，在不用任何方法的时候很难进行一个综合评价，所以需要层次分析法。<br>所以在面对多属性决策问题的时候，需要这些算法。</p><h2 id="1-2-数据链评估"><a href="#1-2-数据链评估" class="headerlink" title="1.2 数据链评估"></a>1.2 数据链评估</h2><p>数据链评估问题，带入到多属性决策模型中，就是面临数据链通信性能指标很多和很复杂的情况，需要一种数学算法来综合这些指标的参数，最后得到一个对多种抗干扰方案的评分和排序，给决策者提供参考。<br>那么我们需要思考的是，在数据链评估问题中是否满足这样的模型。<br>首先如果仅针对数据链抗干扰性能，狭义上我认为只需要得到数据链经过干扰前后的<strong>误码率性能</strong>就可以代表数据链抗干扰性能，那么在这种条件下就不需要使用任何数学算法。<br>在更广的意义上，也可以将其他的一些指标引入，以评估数据链的抗干扰性能，例如功率抑制度、频率抑制度、同步时间等，但是我认为大部分指标较为意义不明，功率的抑制或频率的抑制最终对数据链抗干扰能力的影响，也是体现在误码率上的，有误码率就足以代替掉这些指标。<br>我的结论是数据链抗干扰性能不具备建模成多属性决策模型的意义，也不需要数学算法来对其进行评估，因为误码率性能就足以代表数据链的抗干扰性能，事实上很多情况下(比如对数据链进行抗干扰技术的改进之后，也是用误码率性能的提升来体现该算法的好坏)也是就这么做的。<br>拓展到数据链评估，我认为就具有进行评估的意义了。</p><blockquote><p>对通信系统来说,衡量其通信性能的指标有很多,包括有效性、可靠性、适应性、标准性、可维护性以及经济性等,其中最重要的两个性能指标就是有效性和可靠性。</p><blockquote><p>基于强化学习的机载通信波形参数决策方法研究_赵思敏   </p></blockquote></blockquote><p>可以预见的，在拓展的评估对象的范围之后，可以选择的指标变多，可以选择有效性、可靠性、适应性、标准性、可维护性以及经济性等指标。基本满足多属性决策的建模要求，但是除了有效性和可靠性指标之外的指标，无法通过仿真获得，所以事实上能够选择的指标还是非常有限，所以我认为做数据链评估有一定的意义，但意义不大。</p><h2 id="1-3-评估方法"><a href="#1-3-评估方法" class="headerlink" title="1.3 评估方法"></a>1.3 评估方法</h2><p>已知数据链评估有一定意义，但意义不大，所以现有的研究中，该方向的文献比较少，只考虑数学算法，目前现有的数据链评估在层次分析法、TOPSIS法阶段。<br>现有的雷达效能评估处于TOPSIS法、直觉模糊评估阶段。<br>而多属性决策方法目前最新的研究进展是基于q阶模糊集的一些评估方法。引入q阶模糊集的原因是，例如误码率曲线在之前的评估中只用了一个参数代表其效果，这是不合理的，所以在q阶模糊集中，引入了隶属度和非隶属度两个参数来代表一条误码率曲线的性能。<br>然而无论使用什么评估方法，最终的结果都是几个评估值和一个排序，只能自己看图说明该评估方法的优越性，例如敏感性等，并且这样的评估最后在决策时只能列表查询，灵活性比较低。<strong>重要的是</strong>无论用什么数学方法，最后得到的都是一个评分值，难以说明算法的优势，最合理的是从原理上说明。</p><h1 id="2-数据链参数决策"><a href="#2-数据链参数决策" class="headerlink" title="2.数据链参数决策"></a>2.数据链参数决策</h1><h2 id="2-1-评估到决策"><a href="#2-1-评估到决策" class="headerlink" title="2.1 评估到决策"></a>2.1 评估到决策</h2><p>数据链评估的流程，是已经有很多抗干扰方案，例如不同的调制方式、编码方式、跳频策略等，然后分别对这些方案在不同的信道环境下进行评估，得到很多评分值，最终形成一个表格，在决策者面对不同的信道环境时，查表选择不同的抗干扰方案。这个查表操作在3GPP长期演进(LTE)系统中的AMC方案里面的调制编码方案的集合(MCS)有点类似，将在后续的章节里进行介绍。<br>上述介绍的多属性决策过程，实际上就是建立一个可供参考的表格的过程，这个过程中的决策者是人。这样的方法，最终的结果都是得到一个排序或者是表格，所有的方法几乎可以获得相同的效果，那么这个议题是否有意义就成为了问题。<br>我认为，为了达到选择最优数据链抗干扰方案的目的，并非一定要研究评估打分，可以直接考虑研究决策方法。<br>评估和决策的关系我们可以参考下图：  </p><img src="/2024/04/25/%E9%80%9A%E4%BF%A1%E6%8A%97%E5%B9%B2%E6%89%B0%E7%AD%96%E7%95%A5_%E4%BB%8E%E8%AF%84%E4%BC%B0%E5%88%B0%E5%86%B3%E7%AD%96/%E5%86%B3%E7%AD%96%E7%B3%BB%E7%BB%9F%E5%9B%BE.png" class="px" title="决策系统图"> <blockquote><p> 基于强化学习的机载通信波形参数决策方法研究_赵思敏  </p></blockquote><p>上图是自适应调制编码与扩频决策系统图，我们之前的评估，所做的工作就是图中的<strong>误码率通信性能计算</strong>部分，就是为了得到一个评分。该文章中直接用误码率性能来代表了数据链的性能，我们之前是在误码率性能之外又找了其他的指标，再利用数学方法得出一个评分，其实直接用误码率代表性能也是完全没问题的。<br>图中的指导决策部分是可以研究的方向。</p><h2 id="2-2-AMC"><a href="#2-2-AMC" class="headerlink" title="2.2 AMC"></a>2.2 AMC</h2><blockquote><p>传统的波形参数决策主要是对调制方式和编码速率的决策,可看作是AMC技术。在传统的AMC技术中,根据信道的状态信息(ChannelStateInformation,CSI),实时地调整调制方式和编码速率最大化系统吞吐量。AMC技术实现主要分为三个步骤:信道状态估计、信噪比估计、MCS切换方案。其中,信道状态估计主要是通过非盲信道、盲信道以及半盲信道估计等方法获取当前信道的估计参数;信噪比估计是将信道的状态信息映射为信噪比(SignalNoiseRatio,SNR)值,目前主要的研究方法有最大似然估计、基于谱估计、基于统计量估计等;MCS切换方案是指根据估计出来的信道状态信息和信噪比值来选择对应的调制方案,目前主要的研究方法有基于固定门限和基于门限调整。</p><blockquote><p>基于强化学习的D2D波形参数决策方法研究_谢霞  </p></blockquote></blockquote><p>AMC系统的框图如下所示  </p><img src="/2024/04/25/%E9%80%9A%E4%BF%A1%E6%8A%97%E5%B9%B2%E6%89%B0%E7%AD%96%E7%95%A5_%E4%BB%8E%E8%AF%84%E4%BC%B0%E5%88%B0%E5%86%B3%E7%AD%96/%E8%87%AA%E9%80%82%E5%BA%94%E8%B0%83%E5%88%B6%E7%BC%96%E7%A0%81%E7%B3%BB%E7%BB%9F%E6%A1%86%E5%9B%BE.png" class="" title="自适应调制编码系统框图"><p>传统的AMC方法使用的就是查表，也就是建立MCS的方法来进行自适应调制和编码的调整</p><p>多目标基础函数是一种层次分析法，可以用评估方法代替。</p>]]></content>
    
    
    <categories>
      
      <category>抗干扰策略</category>
      
    </categories>
    
    
    <tags>
      
      <tag>强化学习</tag>
      
      <tag>抗干扰策略</tag>
      
      <tag>评估</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>1.C++数据类型</title>
    <link href="/2024/04/24/1.C-%E6%95%B0%E6%8D%AE%E7%B1%BB%E5%9E%8B/"/>
    <url>/2024/04/24/1.C-%E6%95%B0%E6%8D%AE%E7%B1%BB%E5%9E%8B/</url>
    
    <content type="html"><![CDATA[<h1 id="1-数据类型"><a href="#1-数据类型" class="headerlink" title="1. 数据类型"></a>1. 数据类型</h1><p>C++规定在创建一个变量或者常量时，必须要指定出相应的数据类型，否则无法给变量分配内存</p><h2 id="1-1-整型"><a href="#1-1-整型" class="headerlink" title="1.1 整型"></a>1.1 整型</h2><p><strong>作用</strong>：整型变量表示的是整数类型的数据</p><p>C++中能够表示整型的类型有以下几种方式，区别在于所占内存空间不同：</p><table><thead><tr><th>数据类型</th><th>占用空间</th><th>取值范围</th></tr></thead><tbody><tr><td>short(短整型)</td><td>2字节</td><td>(-2^15 ~ 2^15-1)</td></tr><tr><td>int(整型)</td><td>4字节</td><td>(-2^31 ~ 2^31-1)</td></tr><tr><td>long(长整形)</td><td>Windows为4字节，Linux为4字节(32位)，8字节(64位)</td><td>(-2^31 ~ 2^31-1)</td></tr><tr><td>long long(长长整形)</td><td>8字节</td><td>(-2^63 ~ 2^63-1)</td></tr></tbody></table><h2 id="1-2-sozeof关键字"><a href="#1-2-sozeof关键字" class="headerlink" title="1.2 sozeof关键字"></a>1.2 sozeof关键字</h2><p><strong>作用</strong>：利用sizeof关键字可以统计数据类型所占内存大小  </p><p><strong>语法</strong>：<code>sizeof( 数据类型 / 变量)</code>  </p><p><strong>示例</strong>：</p><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><code class="hljs C++"><span class="hljs-function"><span class="hljs-type">int</span> <span class="hljs-title">main</span><span class="hljs-params">()</span> </span>&#123;<br><br>    cout &lt;&lt; <span class="hljs-string">&quot;short 类型所占内存空间为： &quot;</span> &lt;&lt; <span class="hljs-built_in">sizeof</span>(<span class="hljs-type">short</span>) &lt;&lt; endl;<br><br>    cout &lt;&lt; <span class="hljs-string">&quot;int 类型所占内存空间为： &quot;</span> &lt;&lt; <span class="hljs-built_in">sizeof</span>(<span class="hljs-type">int</span>) &lt;&lt; endl;<br><br>    cout &lt;&lt; <span class="hljs-string">&quot;long 类型所占内存空间为： &quot;</span> &lt;&lt; <span class="hljs-built_in">sizeof</span>(<span class="hljs-type">long</span>) &lt;&lt; endl;<br><br>    cout &lt;&lt; <span class="hljs-string">&quot;long long 类型所占内存空间为： &quot;</span> &lt;&lt; <span class="hljs-built_in">sizeof</span>(<span class="hljs-type">long</span> <span class="hljs-type">long</span>) &lt;&lt; endl;<br><br>    <span class="hljs-built_in">system</span>(<span class="hljs-string">&quot;pause&quot;</span>);<br><br>    <span class="hljs-keyword">return</span> <span class="hljs-number">0</span>;<br>&#125;<br></code></pre></td></tr></table></figure><blockquote><p>整型结论：short &lt; int &lt;&#x3D; long &lt;&#x3D; long long  </p></blockquote><p><strong>问题</strong>：这里遇到了控制台输出中文乱码的问题，原因是控制台输出的字符格式没有被定义为UTF-8 编码<br><strong>解决方法</strong>：设置控制台输出为UTF-8 编码格式，具体操作如下：</p><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><code class="hljs C++"><span class="hljs-meta">#<span class="hljs-keyword">include</span> <span class="hljs-string">&lt;Windows.h&gt;</span></span><br><span class="hljs-function"><span class="hljs-type">int</span> <span class="hljs-title">main</span><span class="hljs-params">()</span></span><br><span class="hljs-function"></span>&#123;<br>   <span class="hljs-built_in">SetConsoleOutputCP</span>(CP_UTF8);  <span class="hljs-comment">// 设置控制台输出为 UTF-8 编码</span><br>   cout &lt;&lt; <span class="hljs-string">&quot;你好 &quot;</span>&lt;&lt; endl;<br>&#125;<br></code></pre></td></tr></table></figure><h2 id="1-3-实型（浮点型）"><a href="#1-3-实型（浮点型）" class="headerlink" title="1.3 实型（浮点型）"></a>1.3 实型（浮点型）</h2><p><strong>作用</strong>：用于表示小数<br>浮点型变量分为两种：  </p><ol><li>单精度float</li><li>双精度double</li></ol><p>两者的区别在于表示的有效数字范围不同。  </p><table><thead><tr><th>数据类型</th><th>占用空间</th><th>有效数字范围</th></tr></thead><tbody><tr><td>float</td><td>4字节</td><td>7位有效数字</td></tr><tr><td>double</td><td>8字节</td><td>15～16位有效数字</td></tr><tr><td><strong>示例：</strong></td><td></td><td></td></tr></tbody></table><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><code class="hljs C++"><span class="hljs-function"><span class="hljs-type">int</span> <span class="hljs-title">main</span><span class="hljs-params">()</span> </span>&#123;<br><br>    <span class="hljs-type">float</span> f1 = <span class="hljs-number">3.14f</span>;<br>    <span class="hljs-type">double</span> d1 = <span class="hljs-number">3.14</span>;<br><br>    cout &lt;&lt; f1 &lt;&lt; endl;<br>    cout &lt;&lt; d1&lt;&lt; endl;<br><br>    cout &lt;&lt; <span class="hljs-string">&quot;float  sizeof = &quot;</span> &lt;&lt; <span class="hljs-built_in">sizeof</span>(f1) &lt;&lt; endl;<span class="hljs-comment">//4字节</span><br>    cout &lt;&lt; <span class="hljs-string">&quot;double sizeof = &quot;</span> &lt;&lt; <span class="hljs-built_in">sizeof</span>(d1) &lt;&lt; endl;<span class="hljs-comment">//8字节</span><br><br>    <span class="hljs-comment">//科学计数法</span><br>    <span class="hljs-type">float</span> f2 = <span class="hljs-number">3e2</span>; <span class="hljs-comment">// 3 * 10 ^ 2 </span><br>    cout &lt;&lt; <span class="hljs-string">&quot;f2 = &quot;</span> &lt;&lt; f2 &lt;&lt; endl;<br><br>    <span class="hljs-type">float</span> f3 = <span class="hljs-number">3e-2</span>;  <span class="hljs-comment">// 3 * 0.1 ^ 2</span><br>    cout &lt;&lt; <span class="hljs-string">&quot;f3 = &quot;</span> &lt;&lt; f3 &lt;&lt; endl;<br><br>    <span class="hljs-built_in">system</span>(<span class="hljs-string">&quot;pause&quot;</span>);<br><br>    <span class="hljs-keyword">return</span> <span class="hljs-number">0</span>;<br>&#125;<br></code></pre></td></tr></table></figure><h2 id="1-4-字符型"><a href="#1-4-字符型" class="headerlink" title="1.4 字符型"></a>1.4 字符型</h2><p><strong>作用:</strong> 字符型变量用于显示单个字符<br><strong>语法:</strong>  <code>char ch = &#39;a&#39;;</code></p><blockquote><p>注意1：在显示字符型变量时，用单引号将字符括起来，不要用双引号  </p></blockquote><blockquote><p>注意2：单引号内只能有一个字符，不可以是字符串</p></blockquote><ul><li>C和C++中字符型变量只占用1个字节。</li><li>字符型变量并不是把字符本身放到内存中存储，而是将对应的ASCII编码放入到存储单元</li></ul><p><strong>示例：</strong> </p><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><code class="hljs C++"><span class="hljs-function"><span class="hljs-type">int</span> <span class="hljs-title">main</span><span class="hljs-params">()</span> </span>&#123;<br>        <br>    <span class="hljs-type">char</span> ch = <span class="hljs-string">&#x27;a&#x27;</span>;<br>    cout &lt;&lt; ch &lt;&lt; endl;<br>    cout &lt;&lt; <span class="hljs-built_in">sizeof</span>(<span class="hljs-type">char</span>) &lt;&lt; endl;<br><br>    <span class="hljs-comment">//ch = &quot;abcde&quot;; //错误，不可以用双引号</span><br>    <span class="hljs-comment">//ch = &#x27;abcde&#x27;; //错误，单引号内只能引用一个字符</span><br><br>    cout &lt;&lt; (<span class="hljs-type">int</span>)ch &lt;&lt; endl;  <span class="hljs-comment">//查看字符a对应的ASCII码</span><br>    ch = <span class="hljs-number">97</span>; <span class="hljs-comment">//可以直接用ASCII给字符型变量赋值</span><br>    cout &lt;&lt; ch &lt;&lt; endl;<br><br>    <span class="hljs-built_in">system</span>(<span class="hljs-string">&quot;pause&quot;</span>);<br><br>    <span class="hljs-keyword">return</span> <span class="hljs-number">0</span>;<br>&#125;<br></code></pre></td></tr></table></figure><h2 id="1-5-转义字符"><a href="#1-5-转义字符" class="headerlink" title="1.5 转义字符"></a>1.5 转义字符</h2><p><strong>作用：</strong> 用于表示一些不能显示出来的ASCII字符<br>现阶段我们常用的转义字符有：<code> \n \\ \t</code>  </p><table><thead><tr><th>转义字符</th><th>含义</th><th>ASCII码值（十进制）</th></tr></thead><tbody><tr><td>\a</td><td>警报</td><td>007</td></tr><tr><td>\b</td><td>退格(BS) ，将当前位置移到前一列</td><td>008</td></tr><tr><td>\f</td><td>换页(FF)，将当前位置移到下页开头</td><td>012</td></tr><tr><td>\n</td><td>换行(LF) ，将当前位置移到下一行开头</td><td>010</td></tr><tr><td>\r</td><td>回车(CR) ，将当前位置移到本行开头</td><td>013</td></tr><tr><td>\t</td><td>水平制表(HT) （跳到下一个TAB位置）</td><td>009</td></tr><tr><td>\v</td><td>垂直制表(VT)</td><td>011</td></tr><tr><td>\ \</td><td>代表一个反斜线字符””</td><td>092</td></tr><tr><td>’</td><td>代表一个单引号（撇号）字符</td><td>039</td></tr><tr><td>“</td><td>代表一个双引号字符</td><td>034</td></tr><tr><td>?</td><td>代表一个问号</td><td>063</td></tr><tr><td>\0</td><td>数字0</td><td>000</td></tr><tr><td>\ddd</td><td>8进制转义字符，d范围0~7</td><td>3位8进制</td></tr><tr><td>\xhh</td><td>16进制转义字符，h范围0<del>9，a</del>f，A~F</td><td>3位16进制</td></tr><tr><td><strong>示例：</strong></td><td></td><td></td></tr></tbody></table><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><code class="hljs C++"><span class="hljs-function"><span class="hljs-type">int</span> <span class="hljs-title">main</span><span class="hljs-params">()</span> </span>&#123;<br>        <br>        <br>    cout &lt;&lt; <span class="hljs-string">&quot;\\&quot;</span> &lt;&lt; endl;<span class="hljs-comment">//输出单个反斜杠</span><br>    cout &lt;&lt; <span class="hljs-string">&quot;\tHello&quot;</span> &lt;&lt; endl;<span class="hljs-comment">//输出tab</span><br>    cout &lt;&lt; <span class="hljs-string">&quot;\n&quot;</span> &lt;&lt; endl;<span class="hljs-comment">//输出换行</span><br><br>    <span class="hljs-built_in">system</span>(<span class="hljs-string">&quot;pause&quot;</span>);<br><br>    <span class="hljs-keyword">return</span> <span class="hljs-number">0</span>;<br>&#125;<br></code></pre></td></tr></table></figure><h2 id="1-6-字符串型"><a href="#1-6-字符串型" class="headerlink" title="1.6 字符串型"></a>1.6 字符串型</h2><p><strong>作用：</strong> 用于表示一串字符<br><strong>两种风格</strong>  </p><ol><li><strong>C风格字符串：</strong> <code>char 变量名[] = &quot;字符串值&quot;</code></li></ol><p>示例  </p><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><code class="hljs C++"><span class="hljs-function"><span class="hljs-type">int</span> <span class="hljs-title">main</span><span class="hljs-params">()</span> </span>&#123;<br><br>    <span class="hljs-type">char</span> str1[] = <span class="hljs-string">&quot;hello world&quot;</span>;<br>    cout &lt;&lt; str1 &lt;&lt; endl;<br>    <br>    <span class="hljs-built_in">system</span>(<span class="hljs-string">&quot;pause&quot;</span>);<br><br>    <span class="hljs-keyword">return</span> <span class="hljs-number">0</span>;<br>&#125;<br></code></pre></td></tr></table></figure><blockquote><p>注意：C风格的字符串要用双引号括起来<br>2. <strong>C++风格字符串：</strong> <code>string 变量名 = &quot;字符串值&quot;</code><br>示例  </p></blockquote><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><code class="hljs C++"><span class="hljs-function"><span class="hljs-type">int</span> <span class="hljs-title">main</span><span class="hljs-params">()</span> </span>&#123;<br><br>    string str = <span class="hljs-string">&quot;hello world&quot;</span>;<br>    cout &lt;&lt; str &lt;&lt; endl;<br>        <br>    <span class="hljs-built_in">system</span>(<span class="hljs-string">&quot;pause&quot;</span>);<br><br>    <span class="hljs-keyword">return</span> <span class="hljs-number">0</span>;<br>&#125;<br></code></pre></td></tr></table></figure><blockquote><p>注意：C++风格字符串，需要加入头文件#include<string></p></blockquote><h2 id="1-7-布尔类型-bool"><a href="#1-7-布尔类型-bool" class="headerlink" title="1.7 布尔类型 bool"></a>1.7 布尔类型 bool</h2><p><strong>作用：</strong> 布尔数据类型代表真或假的值<br>bool类型只有两个值：</p><ul><li>true — 真（本质是1）</li><li>false — 假（本质是0）</li></ul><p><strong>bool类型占1个字节大小</strong><br>示例：  </p><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><code class="hljs C++"><span class="hljs-function"><span class="hljs-type">int</span> <span class="hljs-title">main</span><span class="hljs-params">()</span> </span>&#123;<br><br>    <span class="hljs-type">bool</span> flag = <span class="hljs-literal">true</span>;<br>    cout &lt;&lt; flag &lt;&lt; endl; <span class="hljs-comment">// 1</span><br><br>    flag = <span class="hljs-literal">false</span>;<br>    cout &lt;&lt; flag &lt;&lt; endl; <span class="hljs-comment">// 0</span><br>    <span class="hljs-comment">//本质上 1代表真，0代表假</span><br>    cout &lt;&lt; <span class="hljs-string">&quot;size of bool = &quot;</span> &lt;&lt; <span class="hljs-built_in">sizeof</span>(<span class="hljs-type">bool</span>) &lt;&lt; endl; <span class="hljs-comment">//1</span><br>        <br>    <span class="hljs-built_in">system</span>(<span class="hljs-string">&quot;pause&quot;</span>);<br><br>    <span class="hljs-keyword">return</span> <span class="hljs-number">0</span>;<br>&#125;<br></code></pre></td></tr></table></figure><h2 id="1-8-数据的输入"><a href="#1-8-数据的输入" class="headerlink" title="1.8 数据的输入"></a>1.8 数据的输入</h2><p><strong>作用：</strong> 用于从键盘获取数据<br><strong>关键字</strong> cin<br><strong>语法：</strong> <code>cin &gt;&gt; 变量</code><br>示例：  </p><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br></pre></td><td class="code"><pre><code class="hljs C++"><span class="hljs-function"><span class="hljs-type">int</span> <span class="hljs-title">main</span><span class="hljs-params">()</span></span>&#123;<br><br>    <span class="hljs-comment">//整型输入</span><br>    <span class="hljs-type">int</span> a = <span class="hljs-number">0</span>;<br>    cout &lt;&lt; <span class="hljs-string">&quot;请输入整型变量：&quot;</span> &lt;&lt; endl;<br>    cin &gt;&gt; a;<br>    cout &lt;&lt; a &lt;&lt; endl;<br><br>    <span class="hljs-comment">//浮点型输入</span><br>    <span class="hljs-type">double</span> d = <span class="hljs-number">0</span>;<br>    cout &lt;&lt; <span class="hljs-string">&quot;请输入浮点型变量：&quot;</span> &lt;&lt; endl;<br>    cin &gt;&gt; d;<br>    cout &lt;&lt; d &lt;&lt; endl;<br><br>    <span class="hljs-comment">//字符型输入</span><br>    <span class="hljs-type">char</span> ch = <span class="hljs-number">0</span>;<br>    cout &lt;&lt; <span class="hljs-string">&quot;请输入字符型变量：&quot;</span> &lt;&lt; endl;<br>    cin &gt;&gt; ch;<br>    cout &lt;&lt; ch &lt;&lt; endl;<br><br>    <span class="hljs-comment">//字符串型输入</span><br>    string str;<br>    cout &lt;&lt; <span class="hljs-string">&quot;请输入字符串型变量：&quot;</span> &lt;&lt; endl;<br>    cin &gt;&gt; str;<br>    cout &lt;&lt; str &lt;&lt; endl;<br><br>    <span class="hljs-comment">//布尔类型输入</span><br>    <span class="hljs-type">bool</span> flag = <span class="hljs-literal">true</span>;<br>    cout &lt;&lt; <span class="hljs-string">&quot;请输入布尔型变量：&quot;</span> &lt;&lt; endl;<br>    cin &gt;&gt; flag;<span class="hljs-comment">//bool类型 只要是非0的都代表真</span><br>    cout &lt;&lt; flag &lt;&lt; endl;<br>    <span class="hljs-built_in">system</span>(<span class="hljs-string">&quot;pause&quot;</span>);<br>    <span class="hljs-keyword">return</span> EXIT_SUCCESS;<br>&#125;<br></code></pre></td></tr></table></figure>]]></content>
    
    
    <categories>
      
      <category>C++笔记</category>
      
    </categories>
    
    
    <tags>
      
      <tag>C++</tag>
      
      <tag>数据类型</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Markdown语法介绍</title>
    <link href="/2024/04/24/Markdown%E8%AF%AD%E6%B3%95%E4%BB%8B%E7%BB%8D/"/>
    <url>/2024/04/24/Markdown%E8%AF%AD%E6%B3%95%E4%BB%8B%E7%BB%8D/</url>
    
    <content type="html"><![CDATA[<h1 id="快捷键"><a href="#快捷键" class="headerlink" title="快捷键"></a>快捷键</h1><table><thead><tr><th align="center">功能</th><th align="center">快捷键</th></tr></thead><tbody><tr><td align="center">加粗</td><td align="center">Ctrl + B</td></tr><tr><td align="center">斜体</td><td align="center">Ctrl + I</td></tr><tr><td align="center">引用</td><td align="center">Ctrl + Q</td></tr><tr><td align="center">插入链接</td><td align="center">Ctrl + L</td></tr><tr><td align="center">插入代码</td><td align="center">Ctrl + K</td></tr><tr><td align="center">插入图片</td><td align="center">Ctrl + G</td></tr><tr><td align="center">提升标题</td><td align="center">Ctrl + H</td></tr><tr><td align="center">有序列表</td><td align="center">Ctrl + O</td></tr><tr><td align="center">无序列表</td><td align="center">Ctrl + U</td></tr><tr><td align="center">横线</td><td align="center">Ctrl + R</td></tr><tr><td align="center">撤销</td><td align="center">Ctrl + Z</td></tr><tr><td align="center">重做</td><td align="center">Ctrl + Y</td></tr></tbody></table><p>在vscode中无法使用上述的快捷键，设置的快捷键为vscode中自带的。</p><h1 id="基本语法"><a href="#基本语法" class="headerlink" title="基本语法"></a>基本语法</h1><h2 id="分级标题"><a href="#分级标题" class="headerlink" title="分级标题"></a>分级标题</h2><p>用#表示标题的级别，#后面要加空格</p><h1 id="一级标题"><a href="#一级标题" class="headerlink" title="一级标题"></a>一级标题</h1><h2 id="二级标题"><a href="#二级标题" class="headerlink" title="二级标题"></a>二级标题</h2><h3 id="三级标题"><a href="#三级标题" class="headerlink" title="三级标题"></a>三级标题</h3><h4 id="四级标题"><a href="#四级标题" class="headerlink" title="四级标题"></a>四级标题</h4><h5 id="五级标题"><a href="#五级标题" class="headerlink" title="五级标题"></a>五级标题</h5><h6 id="六级标题"><a href="#六级标题" class="headerlink" title="六级标题"></a>六级标题</h6><h2 id="链接"><a href="#链接" class="headerlink" title="链接"></a>链接</h2><h3 id="插入本地图片"><a href="#插入本地图片" class="headerlink" title="插入本地图片"></a>插入本地图片</h3><h4 id="插入同级目录下的图片"><a href="#插入同级目录下的图片" class="headerlink" title="插入同级目录下的图片"></a>插入同级目录下的图片</h4><pre><code class="hljs">！[图片描述](/pic.jpg)</code></pre><h4 id="插入下一级目录下的图片"><a href="#插入下一级目录下的图片" class="headerlink" title="插入下一级目录下的图片"></a>插入下一级目录下的图片</h4><pre><code class="hljs">！[图片描述](/pics/pic2.jpg)</code></pre><p>图片描述可以不写</p><h3 id="插入互联网上的图片"><a href="#插入互联网上的图片" class="headerlink" title="插入互联网上的图片"></a>插入互联网上的图片</h3><pre><code class="hljs">！[图片描述](https://pic.downcc.com/upload/2015-9/2015923174024.png)</code></pre><h3 id="自动连接"><a href="#自动连接" class="headerlink" title="自动连接"></a>自动连接</h3><p>Markdown 支持以比较简短的自动链接形式来处理网址和电子邮件信箱，只要是用&lt;&gt;包起来， Markdown 就会自动把它转成链接。也可以直接写，也是可以显示成链接形式的</p><h2 id="分隔线"><a href="#分隔线" class="headerlink" title="分隔线"></a>分隔线</h2><p>可以在一行中用三个以上的星号(*)、减号(-)、底线(_)来建立一个分隔线，行内不能有其他东西。你也可以在星号或是减号中间插入空格</p><hr><p>分隔一</p><hr><p>分隔二</p><hr><p>分隔三</p><h2 id="代码块"><a href="#代码块" class="headerlink" title="代码块"></a>代码块</h2><p>插入程序代码的方式有两种，一种是利用缩进(tab), 另一种是利用英文“&#96;”符号</p><h3 id="代码块-1"><a href="#代码块-1" class="headerlink" title="代码块"></a>代码块</h3><p>缩进 4 个空格或是 1 个制表符。</p><h3 id="行内式"><a href="#行内式" class="headerlink" title="行内式"></a>行内式</h3><p>如果在一个行内需要引用代码，只要用反引号&#96;引起来就好</p><h3 id="多行代码块与语法高亮"><a href="#多行代码块与语法高亮" class="headerlink" title="多行代码块与语法高亮"></a>多行代码块与语法高亮</h3><p>在需要高亮的代码块的前一行及后一行使用三个单反引号“&#96;”包裹，就可以了。</p><figure class="highlight css"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs css"><span class="hljs-selector-tag">i</span> am <span class="hljs-selector-tag">code</span><br></code></pre></td></tr></table></figure><h3 id="代码块里面包含html代码"><a href="#代码块里面包含html代码" class="headerlink" title="代码块里面包含html代码"></a>代码块里面包含html代码</h3><p>在代码区块里面， &amp; 、 &lt; 和 &gt; 会自动转成 HTML 实体，这样的方式让你非常容易使用 Markdown 插入范例用的 HTML 原始码，只需要复制贴上，剩下的 Markdown 都会帮你处理。</p><h2 id="引用"><a href="#引用" class="headerlink" title="引用"></a>引用</h2><p>在被引用的文本前加上&gt;符号，以及一个空格就可以<br>可以用多个&gt;进行嵌套引用</p><blockquote><p>eqhueudq</p><blockquote><p>d1</p><blockquote><p>fwe</p></blockquote></blockquote></blockquote><h2 id="列表"><a href="#列表" class="headerlink" title="列表"></a>列表</h2><h3 id="无序列表"><a href="#无序列表" class="headerlink" title="无序列表"></a>无序列表</h3><p>使用 *，+，- 表示无序列表</p><ul><li>列表文字1</li></ul><ul><li>列表文字2</li></ul><ul><li>列表文字3</li></ul><h3 id="有序列表"><a href="#有序列表" class="headerlink" title="有序列表"></a>有序列表</h3><p>使用数字和一个英文句点表示有序列表。</p><ol><li>列表文字1</li><li>列表文字2</li><li>列表文字3</li></ol><h3 id="无序列表和有序列表同时使用"><a href="#无序列表和有序列表同时使用" class="headerlink" title="无序列表和有序列表同时使用"></a>无序列表和有序列表同时使用</h3><ul><li><ol><li>列表文字1</li></ol></li></ul><ul><li><ol start="2"><li>列表文字2</li></ol></li></ul><p>在使用列表时，只要是数字后面加上英文的点，就会无意间产生列表，比如2017.12.30 这时候想表达的是日期，有些软件把它被误认为是列表。解决方式：在每个点前面加上\就可以了</p><h3 id="表格"><a href="#表格" class="headerlink" title="表格"></a>表格</h3><p>用-和|创建表格，如快捷键模块</p><h2 id="常用技巧"><a href="#常用技巧" class="headerlink" title="常用技巧"></a>常用技巧</h2><h3 id="换行"><a href="#换行" class="headerlink" title="换行"></a>换行</h3><p>方法1: 连续两个以上空格+回车  </p><p>方法2：使用html语言换行标签：</p><h3 id="缩进字符"><a href="#缩进字符" class="headerlink" title="缩进字符"></a>缩进字符</h3><p>不断行的空白格   或  半角的空格   或  全角的空格   或  <br>&amp;nbsp缩进缩进  </p><p>没看懂</p><h3 id="特殊符号"><a href="#特殊符号" class="headerlink" title="特殊符号"></a>特殊符号</h3><p>对于 Markdown 中的语法符号，前面加反斜线\即可显示符号本身。  </p><p>其他特殊字符，示例如下：<br>&#10084;<br>&#10003;<br>&#9728;<br>&#9733;<br>&#9730;<br>&#9742;<br>其他的特殊字符可以在这个网站上查看 <a href="https://unicode-table.com/cn/">https://unicode-table.com/cn/</a></p><h3 id="字体、字号与颜色"><a href="#字体、字号与颜色" class="headerlink" title="字体、字号与颜色"></a>字体、字号与颜色</h3><p>Markdown是一种可以使用普通文本编辑器编写的标记语言，通过类似HTML的标记语法，它可以使普通文本内容具有一定的格式</p><h3 id="Latex数学公式"><a href="#Latex数学公式" class="headerlink" title="Latex数学公式"></a>Latex数学公式</h3><h4 id="行内公式"><a href="#行内公式" class="headerlink" title="行内公式"></a>行内公式</h4><p>使用两个”$”符号引用公式:<br>$J\begin{pmatrix}t\end{pmatrix}&#x3D;A\exp\begin{pmatrix}j\begin{pmatrix}2\pi f_ct+\varphi\end{pmatrix}\end{pmatrix}$</p><h4 id="行间公式"><a href="#行间公式" class="headerlink" title="行间公式"></a>行间公式</h4><p>使用两对“$$”符号引用公式<br>$$<br>b_k\begin{pmatrix}t\end{pmatrix}&#x3D;\sum_{n&#x3D;0}^\infty b_ng_b\begin{pmatrix}t-nT_b\end{pmatrix}<br>$$</p>]]></content>
    
    
    <categories>
      
      <category>博客相关</category>
      
    </categories>
    
    
    <tags>
      
      <tag>Markdown</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>开始hexo_Fluid博客编写</title>
    <link href="/2024/04/16/%E5%BC%80%E5%A7%8Bhexo-Fluid%E5%8D%9A%E5%AE%A2%E7%BC%96%E5%86%99/"/>
    <url>/2024/04/16/%E5%BC%80%E5%A7%8Bhexo-Fluid%E5%8D%9A%E5%AE%A2%E7%BC%96%E5%86%99/</url>
    
    <content type="html"><![CDATA[<h2 id="开始一个博客"><a href="#开始一个博客" class="headerlink" title="开始一个博客"></a>开始一个博客</h2><p>在本地创建的文件夹中（例如myblog），右键git bash here，打开命令行窗口后即可开始操作。<br>ping不上github所以更改端口为7890（clash默认端口），每次必须要打开clash才能部署文章到github上。</p><h2 id="hexo命令"><a href="#hexo命令" class="headerlink" title="hexo命令"></a>hexo命令</h2><h3 id="新建一篇文章"><a href="#新建一篇文章" class="headerlink" title="新建一篇文章"></a>新建一篇文章</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">$ hexo new <span class="hljs-string">&quot;文章标题&quot;</span><br></code></pre></td></tr></table></figure><p>这将会在Hexo博客的 source&#x2F;_posts 目录下创建一个名为 文章标题.md 的Markdown文件。可以使用任何文本编辑器打开这个文件，然后开始编写文章内容。</p><p>每次更改完成之后最好先清除之前生成的内容。</p><h3 id="清除之前生成的内容"><a href="#清除之前生成的内容" class="headerlink" title="清除之前生成的内容"></a>清除之前生成的内容</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">$ hexo clean<br></code></pre></td></tr></table></figure><p>本地预览生成后访问<a href="http://localhost:4000/">http://localhost:4000/</a> 即可查看</p><h3 id="生成本地预览"><a href="#生成本地预览" class="headerlink" title="生成本地预览"></a>生成本地预览</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">$ hexo s<br></code></pre></td></tr></table></figure><h3 id="生成静态文章"><a href="#生成静态文章" class="headerlink" title="生成静态文章"></a>生成静态文章</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">$ hexo generate<br></code></pre></td></tr></table></figure><p>部署文章可能会有延迟，以本地预览效果为准</p><h3 id="部署文章"><a href="#部署文章" class="headerlink" title="部署文章"></a>部署文章</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">$ hexo deploy<br></code></pre></td></tr></table></figure>]]></content>
    
    
    <categories>
      
      <category>博客相关</category>
      
    </categories>
    
    
    <tags>
      
      <tag>hexo</tag>
      
    </tags>
    
  </entry>
  
  
  
  
</search>
